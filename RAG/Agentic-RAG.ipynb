{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_SEARCH_API_KEY=os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
    "GOOGLE_CSE_ID=os.getenv(\"GOOGLE_CSE_ID\")\n",
    "LANGSMITH_API_KEY=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" #enable tracing\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading local model\n",
    "MODEL=\"deepseek-r1:1.5b\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "llm = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = llm | parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"YO, tell me a joke\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1. Make a vector store ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a retreiver with vector store out of a pdf document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"thesis.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(f\"Loaded pdf with {len(pages)} pages\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option A : Simple DocArrayInMemorySearch ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B : Facebook AI Similarity Search (FAISS) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY do this once\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#vectorstore = FAISS.from_documents(documents = pages, embedding = embeddings)\n",
    "#vectorstore.save_local(folder_path='data/',index_name=\"thesis-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(folder_path='data/',\n",
    "                               index_name='thesis-index',\n",
    "                               embeddings=embeddings,\n",
    "                               allow_dangerous_deserialization = True\n",
    "                               )\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"reinforcement learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''Answer the question based on the given context. Incase, you \n",
    "cannot answer the question based SOLELY on the context, reply \"I dont know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=['context','question'])\n",
    "\n",
    "print(prompt.format(\n",
    "    context=\"Hi I am susan.\",\n",
    "    question=\"What is my name?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser\n",
    "print(chain.invoke({\n",
    "    \"context\":\"Hi I am susan.\",\n",
    "    \"question\": \"Was ist meine name?\"\n",
    "}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2. Grading top retrieved document ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM for Grading: grades the top document fetched by the retreiver \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an assistant that evaluates documents for relevance and outputs JSON.\"\n",
    ")\n",
    "\n",
    "# Human message is the dynamic input\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"\"\"Give a binary 'yes' or 'no' to grade whether the document is relevant to the question. \\n\n",
    "    Provide the single binary grade as a JSON with a single key 'grade' and no premable or explanation.\n",
    "    Document: {document}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "\n",
    "#dummy run\n",
    "inputs = {'document': [''], 'question':'This is my question'}\n",
    "formatted_messages = chat_prompt.format_messages(**inputs)\n",
    "print('Prompt to retrieval grader')\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type} : {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_grader = chat_prompt | llm | JsonOutputParser()\n",
    "#retrieval_grader = chat_prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy run\n",
    "question = \"What is the motivation behind this project?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[0].page_content\n",
    "\n",
    "grade = retrieval_grader.invoke({\"question\": question, \"document\": doc_text})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sidebar: processing llm output before parsing for robustness ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex as re\n",
    "# def fetch_json(text):\n",
    "#     match=re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "#     if match:\n",
    "#         return match.group(0)\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text=retrieval_grader.invoke(inputs)\n",
    "# clean_text=fetch_json(raw_text)\n",
    "# json_parser = JsonOutputParser()\n",
    "# if clean_text is not None:\n",
    "#     grade=json_parser.parse(clean_text)\n",
    "# print(grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END sidebar: processing llm output before parsing for robustness ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade['grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retreiver fetched document: {doc_text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Generate RAG response chain    ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    ")\n",
    "\n",
    "# Human message is the dynamic input\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"\"\"Use the following context to answer the question in three sentences max. If you don't know the answer, just say so. \n",
    "Limit your response to a maximum of three sentences.\n",
    "    Question: {question}\n",
    "    Context: {context}                                                 \n",
    "\"\"\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy run\n",
    "docs = retriever.invoke(question)\n",
    "content_docs = \" \".join([doc.page_content for doc in docs])\n",
    "generated_response = rag_chain.invoke({\"question\": question, \"context\": content_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: Check for Hallucinations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a grader assessing whether an answer is supported by a set of facts and outputs JSON.\"\n",
    ")\n",
    "\n",
    "# Human message is the dynamic input\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"\"\"Give a binary 'yes' or 'no' to grade whether the answer is supported by the given set of facts.\n",
    "    Provide the single grade as a JSON with a single key 'grade' and no premable or explanation.\n",
    "    \n",
    "    Answer: {generation}\n",
    "    Documents: {documents}\n",
    "\"\"\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "hallucination_grader = chat_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy test run\n",
    "grade = hallucination_grader.invoke({\"generation\": generated_response, \"documents\": content_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade['grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_msg = SystemMessagePromptTemplate.from_template(\"You are a grader assessing whether an answer resolves a question.\")\n",
    "\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"\"\"Give a binary grade 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the grade as a JSON with a single key 'grade' and no preamble or explanation.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {generation}\n",
    "    \"\"\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "\n",
    "answer_grader = chat_prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy test run\n",
    "answer_grader=answer_grader.invoke({\"question\": question, \"generation\": generated_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grade)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5: set up Google search API ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "# from pydantic import BaseModel, Field, root_validator\n",
    "\n",
    "search = GoogleSearchAPIWrapper(k=3,\n",
    "                                google_api_key=GOOGLE_SEARCH_API_KEY,\n",
    "                                google_cse_id=GOOGLE_CSE_ID)\n",
    "def search_results(query):\n",
    "    return search.results(query, num_results=3)\n",
    "\n",
    "web_search_tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=web_search_tool.invoke({'query':'what is Diwali in 2025?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy run\n",
    "question = \"What is the motivation behind this project?\"\n",
    "results=web_search_tool.invoke({'query':question})\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neccesarry declarations before langgraph ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes: \n",
    "    question: string of the question\n",
    "    generation: LLM generation \n",
    "    web_search: whether to add search \n",
    "    documents: list of retreived documents from vector store\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "#node\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args: \n",
    "        state (dict) : Current graph state\n",
    "\n",
    "    Returns: \n",
    "        state (dict): New key added to state: \"documents\". This key contains retrieved documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----RETRIEVE----\")\n",
    "    question = state['question']\n",
    "\n",
    "    #retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {'documents':documents, 'question':question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state: \"generation\". This key contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"----GENERATE----\")\n",
    "\n",
    "    question=state['question']\n",
    "    documents=state['documents']\n",
    "\n",
    "    #RAG generation \n",
    "    generation = rag_chain.invoke({'context':documents,'question':question})\n",
    "    return {'documents':documents,\n",
    "            'question':question,\n",
    "            'generation':generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs=[]\n",
    "    web_search='No'\n",
    "\n",
    "    for d in documents:\n",
    "        answer = retrieval_grader.invoke(\n",
    "            {'question':question,'document':d.page_content}\n",
    "        )\n",
    "\n",
    "        grade = answer['grade']\n",
    "        \n",
    "        #check if document is relevant\n",
    "        if grade.lower() == 'yes':\n",
    "            print(\"--GRADE: DOCUMENT RELEVANT--\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--GRADE: DOCUMENT NOT!!!! RELEVANT--\")\n",
    "            web_search='Yes'\n",
    "            continue\n",
    "    \n",
    "    return {'documents':filtered_docs,\n",
    "            'question':question,\n",
    "            'web_search':web_search}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----WEB SEARCH----\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"snippet\"] for d in docs])\n",
    "\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "    \n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    response = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = response[\"grade\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RETRY---\")\n",
    "        return \"not supported\"  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 6 : Build a graph using langgraph ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\",\"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\" : \"websearch\",\n",
    "        \"generate\" : \"generate\"\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\",\"generate\") #????\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\" : \"generate\",\n",
    "        \"useful\" : END,\n",
    "        \"not useful\" : \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"What is motivation behind this project?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### installations ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install faiss-cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
