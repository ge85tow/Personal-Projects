{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_SEARCH_API_KEY=os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
    "GOOGLE_CSE_ID=os.getenv(\"GOOGLE_CSE_ID\")\n",
    "LANGSMITH_API_KEY=os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading local model\n",
    "MODEL=\"deepseek-r1:1.5b\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "llm = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = llm | parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Sure! Here's a light-hearted joke for you:\n",
      "\n",
      "Why don’t skeletons fight each other?  \n",
      "Because they don’t have the *gym*! 😄\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"YO, tell me a joke\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1. Make a vector store ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pdf with 65 pages\n"
     ]
    }
   ],
   "source": [
    "#making a retreiver with vector store out of a pdf document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"thesis.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(f\"Loaded pdf with {len(pages)} pages\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option A : Simple DocArrayInMemorySearch ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B : Facebook AI Similarity Search (FAISS) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY do this once\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#vectorstore = FAISS.from_documents(documents = pages, embedding = embeddings)\n",
    "#vectorstore.save_local(folder_path='data/',index_name=\"thesis-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(folder_path='data/',\n",
    "                               index_name='thesis-index',\n",
    "                               embeddings=embeddings,\n",
    "                               allow_dangerous_deserialization = True\n",
    "                               )\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3ee8b4a9-b3cf-4e19-b88a-48001ca1dd55', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 20, 'page_label': '14'}, page_content='3 Methodology\\na custom dataset within the Blocksworld domain. Usually, problem statements from\\nBlocksworlds domain consist of a set of named blocks that can be stacked on top of one\\nanother or placed on a table. The goal is to move the blocks from an initial configuration\\nto a target configuration using a sequence of valid actions, namely:pick-up, stack,\\nput-down, unstack. Any action can only be performed after required pre-conditions\\nhave been met, upon performing the action the relevant state variables are updated\\naccordingly to reflect the new configuration. Though the rules are simple, solving a\\nBlocksworld task often requires several steps of planning, making it a useful test-bed\\nfor evaluating how well models can reason through intermediate goals to reach a final state.\\nWe are, by no means, the first ones to use Blocksworld domain problems for test-\\ning reasoning abilities of models. The domain has been widely used in classical planning\\nresearch (Fikes and Nilsson 1971), and more recently in learning-based approaches where\\nmodels attempt to generate action sequences or reason through symbolic transitions\\n(Asai and Fukunaga 2018; Y. Zhou, Paduraru, Boteanu, et al. 2020). It has also featured\\nin neurosymbolic settings that aim to integrate neural networks with planning algorithms\\n(Valmeekam, Zhan, Brafman, et al. 2022). Along with program synthesis tasks requiring\\ninterpretable intermediate steps (Lample and Charton 2019). This consistent use across\\ndecades and methodologies makes Blocksworld a reliable benchmark for evaluating rea-\\nsoning models. A more detailed assessment on this topic can be found in the chapter\\npertaining to related work in chapter 2.\\n3.2.2 Blocksworld Dataset Generation\\nAs the blocksworld domain is well-known there are pre-exisitng datasets that have been\\nused for various studies, as mentioned above. This is freely available to use for academic\\npurposes. Although there’s different versions of this dataset available, they are not suffi-\\nciently vast enough for our purposes. We intend to fine-tune LLMs and further proceed\\nwith Reinforcement Learning. Both of these paradigms require an extensive amount of\\ndata points to get ahead of issues such as overfitting and to ensure generalization across\\ndiverse problem configurations. The limited size of available datasets thus would have\\nrestricted the robustness and reliability of our experiments. This emphasizes the need for\\neither synthetic data generation or augmentation methods to support our experimental\\nobjectives.\\nOwing to this insight, we proceed to create our own blocksworld dataset with namely:\\n• 3 bock configurations\\n14'),\n",
       " Document(id='5fa2a363-5716-49df-b9aa-33ee3d4ead6e', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 13, 'page_label': '7'}, page_content='2 Literature Survey\\nthe model (Y. Xie, Goyal, W. Zheng, et al. 2024).\\nAnother upcoming way to make use of external verifiers is through Reinforcement\\nLearning with Verified Rewards (RLVR) (Su, Yu, Song, et al. 2025; Yue, Z. Chen, R. Lu,\\net al. 2025; Y. Wang, Q. Yang, Zeng, et al. 2025). In this setup, there is no separate\\ntrained reward model. The signal is directly gathered from an existing deterministic\\nheuristic. This is an approach we also adopt and will discuss in more detail later. It\\nworks well in our case because we are working with classical planning problems, where\\nestablished libraries can be used to evaluate plans reliably. A notable concern with RLVR,\\nas pointed out by existing work, is that while it improves sampling efficiency, it does\\nnot significantly expand underlying reasoning abilities of the base model (Yue, Z. Chen,\\nR. Lu, et al. 2025). Whether we reach the coveted ’AHA’ moment as claimed by the\\nDeepSeek team with this approach, still remains an open question.\\n2.3 Evaluating Reasoning in Language Reasoning Models\\n(LRM)\\nIn previous sections, we briefly touched upon the question of LLMs genuinely reason, or\\nmerely pattern-match against their training data. This section delves deeper into this\\nquestion and explores results seen using different methodologies already proposed in the\\nexisting literature to probe reasoning abilities in both LLMs and their more new and\\nimproved version: Language Reasoning Model (LRM).\\n2.3.1 LLMs: Can they plan?\\nCentral topics of discussion in this paper is to test whether LLMs are capable of planning.\\nAs seen earlier, while LLMs have demonstrated strong performance on a range of tasks,\\nthere is still scepticism on their planning abilities. Some even deem the models to simply\\nbe universal approximate retrieval n-gram models (ref:can llms plan?). This argument\\nclaims that LLMs are not solving tasks with guarantee and rather retrieving approximate\\nguesses for solution. To tackle this scepticism, studies have attempted to put critique\\ninto the training loop. The role of generating this critique, was also assigned to an\\nLLM. In such scenarios where LLMs are encouraged to reason through self-critiquing\\n(Valmeekam, Marquez, and Kambhampati 2023; Weng, H. Guo, H. Liu, et al. 2022) while\\none group claims that the performance improves, another experienced a stark opposite.\\nThis divergence highlights that the effectiveness of self-critiquing, and more broadly, the\\nplanning capabilities of LLMs, can vary significantly depending on the task formulation,\\nmodel architecture, training objective, and evaluation method. Therefore, there have\\nbeen continued efforts made to improve the reasoning abilities of LLMs and consequently\\n7'),\n",
       " Document(id='01a4686e-0482-46ce-8d52-7100e1d75832', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 19, 'page_label': '13'}, page_content='3 Methodology\\n3.1 Introduction\\nIn this chapter we begin by presenting a brief overview of the dataset generation, outlining\\nour motivation behind it, the process we follow and finally the resulting distribution.\\nThis is followed by the model selection process and criterion behind the eventual selec-\\ntion. Since evaluating an LLM response requires checking action correctness and plan\\nvalidity, next we describe how responses are processed. For this, we rely heavily on the\\nunified-planning environment which we will briefly touch upon.\\nThis chapter further elaborates on the methodology we follow in this work across the\\nthree main experimental setups: prompting, Supervised Fine-Tuning, and Reinforcement\\nLearning. The objective is to understand how each of these approaches contributes to\\nreasoning and plan generation in the Blocksworld domain. To this regard, we describe the\\nSFT setup, including pre-processing steps, model input formatting and training details.\\nLastly, we look at the RL setup, more specifically Group Relative Policy Optimization,\\nexplaining how the environment, reward structure, and learning objectives are defined.\\nAdditionally, we also outline the structure and role of different variations of one-shot\\nprompts in producing action plans from language models.\\n3.2 Dataset Generation\\nEach of the following sections builds upon the dataset introduced in its previous section\\nand, together they provide a basis to evaluate the model’s planning ability.\\n3.2.1 Introduction\\nBlocksworld is a legacy environment used to study planning and reasoning. It is a\\nwell-defined domain and yet it contains enough complexity to challenge reasoning models.\\nFor example, through sub-goals that can conflict with one another and tasks that natu-\\nrally break down into smaller sub-problems. Given that the objective of our research\\nis to optimise the reasoning capabilities of the selected model, we choose to generate\\n13'),\n",
       " Document(id='2a2c17b4-6110-4351-b249-90408506aa44', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 49, 'page_label': '43'}, page_content='3 Methodology\\nValid Action:Pick up the brown block ❌  \\nUnstack the brown block from on top of the teal block ✅\\nFigure 14:This figure illustrates the same example configuration in our Blocksworld\\ndomain, as seen in a previous section (Figure 4). This time with this example\\nconfiguration we can understand meaning of valid and invalid actions in a\\ngiven state of blocks. To recap quickly: in this configuration, the left state of\\nblocks is initial configuration and the right one is goal configuration. Given\\nthe initial state of blocks, top action is deemed invalid, whereas bottom one is\\nvalid in our domain.\\ndeliberately assign a slightly higher score to distance progressed (Figure 15) compared\\nto valid plan actions. This is done to encourage the model to go beyond just using the\\ncorrect syntax or action format. We want to reward it more for actually planning in a\\nway that moves towards the goal state. After we compute the total plan reward for the\\nmodel’s response, we also calculate the reward for the gold (reference) plan. The model’s\\nscore is then normalised using the gold plan score so that it fits within the set range of 0\\nto 50 points. This helps us fairly compare different responses and ensures consistency\\nacross examples.\\nThis reward is structured in a way to help guide the model to not only make\\ncorrect moves but also to move in the right direction, step by step, toward the final goal.\\nIt avoids rewarding random or irrelevant actions and its designed to ensure the plan is\\nactually useful in solving the task.\\nBonus Reward Function\\nFinally, we can decode the bonus reward function. This function is set in place to reward\\nthe model separately, and heftily for reaching the desired goal state. This extra reward\\nhopefully helps elicit complete, optimal and correct plans by making successful end\\n43')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"reinforcement learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the given context. Incase, you \n",
      "cannot answer the question based SOLELY on the context, reply \"I dont know\".\n",
      "\n",
      "Context: Hi I am susan.\n",
      "Question: What is my name?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''Answer the question based on the given context. Incase, you \n",
    "cannot answer the question based SOLELY on the context, reply \"I dont know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=['context','question'])\n",
    "\n",
    "print(prompt.format(\n",
    "    context=\"Hi I am susan.\",\n",
    "    question=\"What is my name?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out what Susan's name is from the given context. Let me read through the text carefully.\n",
      "\n",
      "The user starts with \"Hi I am susan.\" That sounds like their first name, and it seems like they're using a contraction for \"susanne,\" which means susan in French. So Susanne would be her full name if she's French, but I don't think that's the case here because the question is about her actual name.\n",
      "\n",
      "Wait, but sometimes people just use their first name without the middle names when it's not needed or assumed. If that's the case, then Susan might be referring to herself as \"susanne,\" which is a common contraction for her full name in French-speaking countries like France, Germany, or Italy. So if she says \"Hi I am susan,\" it could mean she's saying hello and her first name is susanne.\n",
      "\n",
      "But I'm not sure about that. Maybe the question is simpler. It asks \"Was ist meine name?\" which means \"What is my name?\" in German. That would be a direct question to confirm their full name. Since Susan is asking this, they might be referring back to her first name or using a contraction.\n",
      "\n",
      "I'm a bit confused here because the context provided doesn't specify whether it's in French or another language. So I need to make an educated guess based on the information given. If she says \"Hi I am susan,\" it could mean she's asking for her name, which is likely \"susanne\" in French. Alternatively, if she's just using a contraction for her name, Susanne might be the full name.\n",
      "\n",
      "Wait, but maybe I'm overcomplicating this. The question is straightforward: what's Susan's name? If the context doesn't specify language, it's safer to assume it's English since that's where most names are given. In that case, \"susanne\" would be her first name in French-speaking countries.\n",
      "\n",
      "I think I should respond by saying that Susan's name is susanne if she's from a French-speaking region, but if not, maybe it's a different contraction or full name.\n",
      "</think>\n",
      "\n",
      "Sus-anne is her name.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "print(chain.invoke({\n",
    "    \"context\":\"Hi I am susan.\",\n",
    "    \"question\": \"Was ist meine name?\"\n",
    "}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2. Grading top retrieved document ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt to retrieval grader\n",
      "system : You are an assistant that evaluates documents for relevance and outputs JSON.\n",
      "human : Give a binary 'yes' or 'no' to grade whether the document is relevant to the question. \n",
      "\n",
      "    Provide the single binary grade as a JSON with a single key 'grade' and no premable or explanation.\n",
      "    Document: ['']\n",
      "    Question: This is my question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LLM for Grading: grades the top document fetched by the retreiver \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an assistant that evaluates documents for relevance and outputs JSON.\"\n",
    ")\n",
    "\n",
    "# Human message is the dynamic input\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"\"\"Give a binary 'yes' or 'no' to grade whether the document is relevant to the question. \\n\n",
    "    Provide the single binary grade as a JSON with a single key 'grade' and no premable or explanation.\n",
    "    Document: {document}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "\n",
    "#dummy run\n",
    "inputs = {'document': [''], 'question':'This is my question'}\n",
    "formatted_messages = chat_prompt.format_messages(**inputs)\n",
    "print('Prompt to retrieval grader')\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type} : {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_grader = chat_prompt | llm | JsonOutputParser()\n",
    "#retrieval_grader = chat_prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy run\n",
    "question = \"What is the motivation behind this project?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[0].page_content\n",
    "\n",
    "grade = retrieval_grader.invoke({\"question\": question, \"document\": doc_text})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sidebar: processing llm output before parsing for robustness ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def fetch_json(text):\n",
    "    match=re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=retrieval_grader.invoke(inputs)\n",
    "clean_text=fetch_json(raw_text)\n",
    "json_parser = JsonOutputParser()\n",
    "if clean_text is not None:\n",
    "    grade=json_parser.parse(clean_text)\n",
    "print(grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out whether the document is relevant to the question about the motivation behind a project. The question is asking for a 'yes' or 'no' response with a JSON object containing the key 'grade'. \n",
      "\n",
      "Looking at the document provided, it's labeled as \"List of Figures\" and has two sections. Section 14 talks about figures illustrating examples in Blocksworld, discussing actions and their validity. Section 15 compares actions to a simulated game using a distance metric. Both sections are about how distances progress towards the goal.\n",
      "\n",
      "The question is about motivation, which is about the purpose or reasoning behind the project's goals. The document discusses how valid and invalid actions are explored, model responses, and metrics for progression. These topics don't directly address why the project started or what motivates it beyond its structure and objectives.\n",
      "\n",
      "So, the document doesn't provide any information that would answer the question about motivation. It focuses on implementation details and methods within the project's framework.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"grade\": \"no\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END sidebar: processing llm output before parsing for robustness ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade['grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retreiver fetched document: {doc_text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Generate RAG response chain    ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    ")\n",
    "\n",
    "# Human message is the dynamic input\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"\"\"Use the following context to answer the question in three sentences max. If you don't know the answer, just say so. \n",
    "Limit your response to a maximum of three sentences.\n",
    "    Question: {question}\n",
    "    Context: {context}                                                 \n",
    "\"\"\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the motivation behind this project. Let me start by looking at the context provided.\n",
      "\n",
      "The context includes several sections from a document, each starting with numbers 14 and 52, followed by definitions or key points. The first part talks about comparing configurations in Blocksworld, showing valid vs invalid actions, and using distance metrics to understand model responses. It mentions that without considering the initial state's top and bottom, certain actions are seen as invalid. This suggests they're trying to validate or refine how these models operate.\n",
      "\n",
      "Then there are figures labeled 10 through 23. Figure 10 starts with an example problem instance in the Blocksworld domain, followed by a plan being acted upon. Figures 11-17 show various configurations and examples, especially comparing initial and goal states. Figure 18 introduces a natural language representation of these states. Figures 19-23 detail the prompt template for generating plans, overall dataset composition, train/test distributions, modeling techniques, and another figure about GRPO's reward functions.\n",
      "\n",
      "Looking at figures 17 and 18: Figure 17 visualizes block configurations moving from initial to goal, while 18 shows natural language descriptions of these states. This seems to be showing the problem domain in both geometric (block layout) and language (words) forms, which is standard for such projects.\n",
      "\n",
      "Now, considering the project's context: they're comparing state transitions, actions, and distance metrics, and using models like GRPO (which stands for GRP-RO or something similar). They also discuss training strategies, different problem sizes (3-block vs 4-block), and various aspects of language modeling and reinforcement learning techniques.\n",
      "\n",
      "The motivation would be likely about improving the models' ability to understand state transitions in a dynamic environment. Maybe they're trying to model complex interactions like blocksworld gameplay where states are important for planning and action evaluation. This is common in robotics, game AI, or planning systems. So, the project's goal could be to enhance the models' performance in such environments by properly handling state validity and transition distances.\n",
      "\n",
      "I also notice that figure 15 talks about distance progress being calculated from valid actions to the goal. This probably ties into wanting to measure how much progress is made towards achieving the goal through valid moves, which can guide training or evaluation strategies.\n",
      "\n",
      "Additionally, using natural language for model responses suggests they're building a bridge between geometric and linguistic models, aiming to make the state descriptions more understandable and useful. The GRPO method likely represents a new approach in reinforcement learning applied to planning, improving efficiency by not needing extensive memory but still maintaining performance.\n",
      "\n",
      "So putting it all together, the motivation is about advancing models' ability to handle dynamic, multi-state environments through proper action validation, progress tracking, and language representation.\n",
      "</think>\n",
      "\n",
      "The motivation behind this project was to enhance the models' capability to manage complex, dynamic environments. By focusing on state transitions, valid actions, and progress metrics, the goal was to improve how models understand and respond to these states. This involved developing a natural language model to bridge geometric and linguistic representations of states, tracking progress through valid actions. The use of distance metrics helped evaluate model performance, while training strategies aimed at efficiency were considered to make the models more adaptable. The project sought to advance reinforcement learning applications in robotics and game AI by refining action handling and planning mechanisms.\n"
     ]
    }
   ],
   "source": [
    "#dummy run\n",
    "docs = retriever.invoke(question)\n",
    "content_docs = \" \".join([doc.page_content for doc in docs])\n",
    "generated_response = rag_chain.invoke({\"context\": content_docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: Check for Hallucinations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a grader assessing whether an answer is supported by a set of facts and outputs JSON.\"\n",
    ")\n",
    "\n",
    "# Human message is the dynamic input\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"\"\"Give a binary 'yes' or 'no' to grade whether the answer is supported by the given set of facts. \\n\n",
    "    Provide the single binary grade as a JSON with a single key 'grade' and no premable or explanation.\n",
    "    \n",
    "    Answer: {generation}\n",
    "    Documents: {documents}\n",
    "\"\"\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "hallucination_grader = chat_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grade': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "#dummy run\n",
    "grade = hallucination_grader.invoke({\"documents\": content_docs, \"generation\": generated_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grade': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "print(grade)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5: set up Google search API ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "# from pydantic import BaseModel, Field, root_validator\n",
    "\n",
    "search = GoogleSearchAPIWrapper(k=3,\n",
    "                                google_api_key=GOOGLE_SEARCH_API_KEY,\n",
    "                                google_cse_id=GOOGLE_CSE_ID)\n",
    "def search_results(query):\n",
    "    return search.results(query, num_results=3)\n",
    "\n",
    "web_search_tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Lack Of Motivation And Enthusiasm: How To Cope | BetterHelp',\n",
       "  'link': 'https://www.google.com/share.google?q=PrBUcpiMIPxj4ItVs',\n",
       "  'snippet': 'It can lead to a lack of achievement motivation, as individuals may not have the energy or drive to tackle new tasks or projects. Burnout may occur in any area\\xa0...'},\n",
       " {'title': 'ADHD Paralysis Is Real: Here Are 8 Ways to Overcome It',\n",
       "  'link': 'https://www.google.com/share.google?q=3A5mBgs8BhHJe3NCD',\n",
       "  'snippet': \"Every item counts toward completing the bigger project—even if it's an easy win! Every item you complete helps build motivation and foster a sense of\\xa0...\"},\n",
       " {'title': \"'The Project' explores Project 2025's origins and goals to reshape ...\",\n",
       "  'link': 'https://www.google.com/share.google?q=pD04zZcJnK5fww6tL',\n",
       "  'snippet': 'May 1, 2025 ... David Graham\\'s new book, \"The Project,\" details the origins of Project 2025 and its sweeping goals to reshape American culture.'}]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy run\n",
    "question = \"What is the motivation behind this project?\"\n",
    "results=web_search_tool.invoke({'query':question})\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neccesarry declarations before langgraph ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes: \n",
    "    question: string of the question\n",
    "    generation: LLM generation \n",
    "    web_search: whether to add search \n",
    "    documents: list of retreived documents from vector store\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "#node\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args: \n",
    "        state (dict) : Current graph state\n",
    "\n",
    "    Returns: \n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----RETRIEVE----\")\n",
    "    question = state['question']\n",
    "\n",
    "    #retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {'documents':documents, 'question':question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"----GENERATE----\")\n",
    "\n",
    "    question=state['question']\n",
    "    documents=state['documents']\n",
    "\n",
    "    #RAG generation \n",
    "    generation = rag_chain.invoke({'context':documents,'question':question})\n",
    "    return {'documents':documents,\n",
    "            'question':question,\n",
    "            'generation':generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \n",
    "    \"\"\"\n",
    "    If any document is not relevant, we set a flag to run web search\n",
    "\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs=[]\n",
    "    web_search_flag='No'\n",
    "\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {'question':question,'document':d.page_content}\n",
    "        )\n",
    "\n",
    "        grade = score['score']\n",
    "        \n",
    "        #document is relevant\n",
    "\n",
    "        if grade.lower() == 'yes':\n",
    "            print(\"--GRADE: DOCUMENT RELEVANT--\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--GRADE: DOCUMENT NOT!!!! RELEVANT--\")\n",
    "            web_search_flag='Yes'\n",
    "            continue\n",
    "    \n",
    "    return {'documents':filtered_docs,\n",
    "            'question':question,\n",
    "            'web_search_flag':web_search_flag}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----WEB SEARCH----\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    #to-do\n",
    "    docs = search.invoke({\"query\": question})\n",
    "    # web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = \"\\n\".join([d[\"snippet\"] for d in docs])\n",
    "\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# Conditional i.e \"if/else\" edge\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to either web search or RAG. \n",
    "    Args: \n",
    "        state (dict): The current graph state\n",
    "    Returns: \n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    print(\"---ROUTE USER QUESTION---\")\n",
    "    question = state['question']\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\":question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "    if source['datasource'] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"  \n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4 : create a langgraph ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GraphState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m workflow = StateGraph(\u001b[43mGraphState\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'GraphState' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### installations ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/akankshachawla/anaconda3/envs/agentic-rag/lib/python3.13/site-packages (from faiss-cpu) (2.3.3)\n",
      "Requirement already satisfied: packaging in /Users/akankshachawla/anaconda3/envs/agentic-rag/lib/python3.13/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
