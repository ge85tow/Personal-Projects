{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8543a54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf4c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "# OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdc59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPEN AI MODEL -> invalid API KEY \n",
    "MODEL=\"gpt-3.5-turbo\"\n",
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "model.invoke(\"YO, whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a857ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"deepseek-r1:1.5b\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81dfef92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\n\\n</think>\\n\\nSure! Here's a light one:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the *gym*!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.invoke(\"YO, tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37177416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c04536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Why did the scarecrow win an award?  \n",
      "Because he was outstanding in his field.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"YO, tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e8d2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 0, 'page_label': 'a'}, page_content='FAKULTÄT FÜR INFORMATIK\\nTECHNISCHE UNIVERSITÄT MÜNCHEN\\nMaster’s Thesis in Informatik\\nPlanning with Large Language Models\\nAkanksha Chawla, 03766791'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 1, 'page_label': 'i'}, page_content='FAKULTÄT FÜR INFORMATIK\\nTECHNISCHE UNIVERSITÄT MÜNCHEN\\nMaster’s Thesis in Informatik\\nPlanning with Large Language Models\\nAuthor: Akanksha Chawla, 03766791\\nSupervisor: Prof. Matthias Grabmair\\nAdvisor: Prof. Matthias Grabmair\\nSubmission Date: Submission date'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 2, 'page_label': 'ii'}, page_content='I confirm that this master’s thesis in informatik is my own work and I have documented\\nall sources and material used.\\nMunich, Submission date Akanksha Chawla, 03766791'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 3, 'page_label': 'iii'}, page_content='Acknowledgments\\nI would like to express my deepest gratitude to everyone who has supported me\\nthroughout my journey to this point in my life.\\nFirst and foremost my, I am greatly indebted to my Guru - my guide, Matthias\\nGrabmair. His invaluable guidance, encouragement and unwavering commitment to\\ncultivating a meaningful research experience for his students has bolstered my confidence\\nin my own abilities time and again. He has helped me in more ways than one: through\\ninsightful feedback, intellectual freedom, and by always making time for my questions,\\nno matter how trivial. His mentorship has left a lasting impact on how I think and learn.\\nThank you, for fuelling my aspiration to grow as a student for life.\\nUnder his mentorship, I had the opportunity to learn from and work alongside capable\\ncolleagues at the TUM Legal Tech Department. A supportive network of curious minds\\nand collaborative spirits.\\nA Master’s thesis is a project that truly brings the phrase \"it takes a village\" to life.\\nFortunately, my village has been enriched with nurturing, generous souls.\\nLong before I could dream, two young engineers were dreaming for me. I start with\\nexpressing my gratitude to the two people in my life who sowed the seed of learning in me,\\nway before I even gained consciousness. My dear parents, who repeatedly fought against\\ntheir own limitations to make my life’s possibilities limitless. Without your momentous\\nefforts and constant sacrifices, I would not be writing this document. Thank you, for\\nalways believing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 4, 'page_label': 'iv'}, page_content='Abstract\\niv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 5, 'page_label': 'v'}, page_content='Contents\\nAcknowledgments iii\\nAbstract iv\\n1 Introduction 1\\n2 Literature Survey 3\\n2.1 Overview of Reasoning with Language Models . . . . . . . . . . . . . . . . 3\\n2.2 Attempts at Eliciting Reasoning . . . . . . . . . . . . . . . . . . . . . . . 4\\n2.3 Evaluating Reasoning in Language Reasoning Models (LRM) . . . . . . . 7\\n2.3.1 LLMs: Can they plan? . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.3.2 Rethinking Current Reasoning Benchmarks . . . . . . . . . . . . . 8\\n2.3.3 Blocksworld as a Reasoning Benchmark . . . . . . . . . . . . . . . 9\\n2.4 Limitations and Research Gaps . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3 Methodology 13\\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2 Dataset Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2.2 Blocksworld Dataset Generation . . . . . . . . . . . . . . . . . . . 14\\n3.2.3 From Dataset to Prompt . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.2.4 Dataset splits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n3.3 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n3.3.1 Selection Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n3.3.2 Mathematical Representation of Planning Problems . . . . . . . . 23\\n3.3.3 Translating Model Output into Executable Plans . . . . . . . . . . 25\\n3.3.4 Building Executable Plans from Actions . . . . . . . . . . . . . . . 25\\n3.3.5 Benchmarking Candidate Models . . . . . . . . . . . . . . . . . . . 25\\n3.4 Prompting Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n3.4.1 Chain-of-Thought Prompting . . . . . . . . . . . . . . . . . . . . . 28\\n3.5 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . 30\\n3.5.1 Overview of SFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n3.5.2 Preparation of Input Data . . . . . . . . . . . . . . . . . . . . . . . 31\\nv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 6, 'page_label': 'vi'}, page_content='Contents\\n3.5.3 SFT Training Implementation . . . . . . . . . . . . . . . . . . . . . 33\\n3.6 Reinforcement learning with GRPO . . . . . . . . . . . . . . . . . . . . . 35\\n3.6.1 Introduction to GRPO . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n3.6.2 Input Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . 38\\n3.6.3 Reward Signal Design . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3.6.4 Training Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n4 Experimentation and Results 48\\n5 Final discussion, Limitations and Future Work 49\\nList of Figures 51\\nList of Tables 53\\nBibliography 54\\nvi'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 7, 'page_label': '1'}, page_content='1 Introduction\\nThe advent of Large Language Models (LLMs) revolutionizing Natural Language tasks\\nand disrupting almost every field that can be automated is the general preface to most\\npapers in this area of research. Since the introduction of the Transformer architecture\\nin the seminal 2017 paper titled “Attention Is All You Need” (Vaswani, Shazeer, Par-\\nmar, et al. 2017), the domain of Artificial Intelligence (AI) has witnessed path-breaking\\nresearch leveraging this architectural advancement. A year following this was Open\\nAI’s first Generative Pre-Trained Transformer (GPT) that marked the birth of LLM as\\nwe now know them. Though these models were originally developed to perform word\\nsequence completion tasks, through numerous iterations and experimentations researchers\\nhave inched closer to harnessing the true potential hidden within these neural network\\nblack boxes. The plethora of AI-led assistants — such as Khanmigo, Notion AI, Cursor,\\nand DeepScribe — is a testament to this progress, showcasing how Transformer-based\\narchitectures have revolutionised human-computer interaction.\\nBut the question we try to explore in this research is: how far can we extend the\\ncapabilities of these models? Even more specifically, what are the limits of their reason-\\ning, planning, and generalisation abilities? As more research dedicated to developing the\\nnext cornerstone in the journey towards channelling the cognitive potential of machines\\nemerges, it becomes increasingly important to examine their planning abilities. At present\\n- no matter how skilled - LLMs still largely act as assistants in human workflows. For\\nLLMs to become truly self-reliant agents, the ability to perform intuitive and context-\\naware planning is indispensable. This interest has led to an outburst of research prodding\\nand probing their behaviour almost as if they are already human-like organism. To this\\nend, planning, as a form of reasoning, has long been a focus within the AI community.\\nAt its core, it’s about figuring out a sequence of actions: a policy, that can take: an\\nagent from where it is now to where it needs to be. Traditionally, planning has been\\napproached as an inference problem over world and reward models. In this paper, we\\nexplore how well large language models can reason about actions and change, specifically\\nin the context of Blocksworld planning tasks.\\nOn this note, we have seen a recent evolution of LLMs into Language Reasoning Models\\n(LRMs). LRM build upon the capabilities of LLMs by essentially making models pause\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 8, 'page_label': '2'}, page_content='1 Introduction\\nto reason before responding. While LLMs are trained to simply predict the next token\\nof generation, LRMs are expected to build reasoning traces leading up to the final\\nanswer, thus making thought-through decisions rather than rushing to the conclusion.\\nIt’s a notion where the model learns throughout its journey, not just at the end as was\\ntraditionally done. Within this context of reasoning capabilities lies OpenAI’s o1 series of\\nmodels, which were the first to demonstrate inference-time scaling simply by increasing\\nthe length of their Chain of Thought (CoT) reasoning process. It is suspected that\\nOpenAI used these longer CoT traces to Supervised Fine-Tuning (SFT) their existing\\nmodels, aligning responses more closely with human preferences. This accomplishment\\nhas achieved significantly impressive results across reasoning tasks such as mathematics,\\ncoding and scientific reasoning. Following this first step on the untouched land, there\\nwas a surge of research exploring different ways to get models to reason—ranging from\\nsimply adding a ’WAIT’ token to encourage more deliberate responses, to using external\\nverifiers as hierarchical reward signals to guide reasoning. Among this group, emerged\\na novel model training algorithm Group Relative Policy Optimization (GRPO) that\\nmade use of the fact that models can understand better than they can generate. The\\nmodel born out of this training algorithm was: DeepSeek. DeepSeek-R1 matched, if\\nnot outperformed, its predecessors on various benchmarks. Unlike larger labs such as\\nOpenAI, the DeepSeek team achieved these results using significantly fewer compute\\nresources, while still leveraging the Reinforcement Learning (RL) paradigm.\\nOur aim in this paper is to experiment with these methods championed by the industry\\nleaders and emerging researchers on a smaller model by Google - Google/gemma-3-12b-\\nit. We intend to do this on a dataset well-known in the planning with AI community:\\nBlocksworld. We have extended this dataset to fit our purposes. Blocksworld is inherently\\nstructured and lends itself easily to reasoning tasks. The planning tasks in this dataset\\ninvolves moving blocks or stacks of blocks across towers to reach a specific desired goal\\nstate. While trivial for humans, the challenge for models lies in the multi-step planning\\nrequired to get there. As model needs to reason step-by-step through these problems.\\nThe crux of our experiment is to see which one of the three: prompting, SFT, or RL, best\\nhelps optimize the model’s reasoning abilities. Furthermore, whether such reasoning can\\nscale even without massive model sizes or proprietary data long form CoT trails. We test\\nif the model can learn to think in terms of intermediary sub-goals, rather than hallucinate\\nor shortcut its way to the final goal state. Each training phase is designed to not only\\nimprove accuracy, but to shape the manner in which the model arrives at its answer.\\nThus, reflecting the larger shift from language prediction to deliberative reasoning.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 9, 'page_label': '3'}, page_content='2 Literature Survey\\nThis chapter presents a survey of existing work related to reasoning with large language\\nmodels and its evaluation in well-structured domains. To maintain a clear flow the\\nliterature studied has been categorised based on the main techniques used. Each section\\ngives an overview of selected papers, highlights key ideas, and explains how they connect\\nto this research project.\\n2.1 Overview of Reasoning with Language Models\\nIn recent years, natural language processing (NLP) has seen an immense degree of evolu-\\ntion, and its success has to be credited to the invention of self-attention and Transformer;\\nboth of which have neural network-based architectures. This advent has led to exploring\\ndifferent techniques to make models work on downstream tasks of NLP including but not\\nlimited to: commonsense reasoning (J. Yang, Jin, Tang, et al. 2023), multiple-choice\\nquestion answering (Robinson, Rytting, and Wingate 2023) and solving math world\\nproblems (Cobbe, Kosaraju, Bavarian, et al. 2021), without having to train these models\\nfrom scratch.\\nA distinctive aspect that sets, the last example in this list, apart from the rest is\\nour main area of interest for this project. Solving tasks like grade-school math problems\\nrequires one to reason to reach the desired solution. In recent literature, researchers\\nare avidly exploring how to get large (and sometimes small) language models to reason.\\nResearch from only couple years ago (Wei, X. Wang, Schuurmans, et al. 2022) introduce\\nChain-of-Thought prompting, where models are guided to reason step-by-step before\\ngiving an answer. This shows strong gains on the above mentioned, math and additionally\\nlogic tasks. This acts as a benchmark for testing arithmetic reasoning abilities of models.\\nThis is fuelled further with proposal of methods (B. Zhang, Y. Shao, Deng, et al. 2022) to\\nautomatically select better reasoning paths from multiple samples. This idea is extended\\nwith self-consistency (X. Wang, Wei, Schuurmans, et al. 2023), where multiple reasoning\\npaths are sampled and the final answer is chosen by majority vote.\\nFollowing the initial breakthroughs, many more such research work has followed. There\\nhave been significant developments in the intersection of LLMs and planning. In this\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 10, 'page_label': '4'}, page_content='2 Literature Survey\\nintersection, LLMs have taken up an array of roles (Kambhampati, Valmeekam, Marquez,\\nand Guan 2023). Roles ranging from generating plans (W. Huang, Abbeel, Pathak,\\nand Mordatch 2022; Valmeekam, Marquez, Sreedharan, and Kambhampati 2023) and\\nheuristics (Valmeekam, Marquez, Sreedharan, and Kambhampati 2023; Ahn, Brohan,\\nBrown, et al. 2022) to eliciting planning knowledge (Guan, Valmeekam, Sreedharan, and\\nKambhampati 2023). Along with efforts to use human feedback from users and/or the\\nenvironment (W. Huang, Xia, Xiao, et al. 2022; Raman, Cohen, Rosen, et al. 2022; Yao,\\nZhao, Yu, et al. 2023) to improve performance. Research has also explored using LLMs\\nas scoring models (Ahn, Brohan, Brown, et al. 2022).\\nEven so, most research in this domain of planning with LLMs still tackle datasets\\nthat focus on rather shallow reasoning. They fail to provide much insight into the true\\nplanning abilities of language models. The most prominent and widely used datasets for\\nevaluating recent advancements in reasoning with language models include: BIG-BENCH\\n(Srivastava, Rastogi, Rao, et al. 2022), GSM8K (Cobbe, Kosaraju, Bavarian, et al. 2021),\\nSVAMP (Patel, Marasovic, Agrawal, et al. 2021), CommonsenseQA (J. Yang, Jin, Tang,\\net al. 2023) and StrategyQA (Geva, Khashabi, Khot, et al. 2021). As the performance\\non these benchmarks is overall good, the methodologies applied to achieve them are\\ntouted in the industry. The most recent such example in this category, are the DeepSeek\\nmodels (DeepSeek-AI, D. Guo, D. Yang, et al. 2025).\\nAs eluded to the previous chapter, our aim in this research is to experiment with these\\nmethods championed by the industry and then evaluate on a dataset well-established in\\nthe domain ofclassical planning. In our domain sub-goals cannot be simply ignored\\nand the planning is more tangible. This helps us measure whether these reasoning\\ntechniques actually translate to structured, symbolic problem settings that unlike other\\ntasks, cannot be hacked by rote-learning. To this end, the next few sections will elaborate\\non the methods we adopt and existing work that motivates them, setting the foundation\\nfor our research.\\n2.2 Attempts at Eliciting Reasoning\\nInference Only Methods\\nA growing body of research explores how to elicit reasoning behaviour from language\\nmodels. The first line of work focuses on inference only methods that do not involve any\\ntraining. These methods use prompting techniques, most notably CoT, to guide models\\nto produce intermediate reasoning steps.\\nFor example the aforementioned, Wei et al. (Wei, X. Wang, Schuurmans, et al. 2022)\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 11, 'page_label': '5'}, page_content='2 Literature Survey\\nshow that adding intermediate reasoning steps in the prompt significantly improves\\nperformance on math and logic benchmarks. Another team of researchers (Kojima,\\nGu, Reid, et al. 2022) extend this further by demonstrating that even simple COT\\nincantations like “Let’s think step by step” help models reason better without the need\\nof any supervised data. Other work such as Wang and Zhou (B. Wang and D. Zhou\\n2022) study conditions under which models can independently generate reasoning steps\\neven without explicit CoT cues.\\nHowever, as Ye and Durrett (Ye and Durrett 2022) show, the quality and reliability\\nof these reasoning trace textual explanations remain inconsistent. This research shows\\nthat model as large as GPT-3 falter in generating sound reasoning traces. While these\\nmethods may seem intriguing the actual impact of these methods on performance for a\\nwell-structured classical planning problem like our dataset, still remains to be evaluated.\\nFurthermore, as hinted at by fellow studies (Wei, X. Wang, Schuurmans, et al. 2022;\\nB. Wang and D. Zhou 2022) COT does not positively impact performance for models\\nsmaller than 100B parameters. Unsurprisingly, while models smaller in size are capable\\nof producing fluent COT, their coherence with respect to the task remains rather ques-\\ntionable.\\nIn our project, we use an instruction-fine-tuned 12B parameter model due to hard-\\nware limitations. This places us well below the 100B+ scale where CoT methods have\\nbeen shown to be most effective (Wei, X. Wang, Schuurmans, et al. 2022; B. Wang and D.\\nZhou 2022). In fact, several studies suggest that CoT has limited or even negative impact\\non smaller models. This makes it important to critically evaluate whether inference-only\\nmethods actually help in our setup.\\nSupervised Approaches for Reasoning\\nNext in line are supervised approaches, where models are fine-tuned on datasets that con-\\ntain explicit example reasoning traces. Unlike inference-only methods, these approaches\\ntry to teach the base model how to reason by directly exposing it to numerous examples\\nof step-by-step thinking during training. Starting with study that offers practical insights\\n(Pareja, Smith, Kumar, and Lee 2024) on how small instruction-tuned models can benefit\\nfrom reasoning-aligned supervision. Followed by research that train verifiers on GSM8K\\n(Cobbe, Kosaraju, Bavarian, et al. 2021), to improve both answer accuracy and reasoning\\nquality. Other works (M. Li, R. Zhang, X. Chen, and Gupta 2023) contradict this and\\nstate that models often rely more on structural patterns in demonstrations than on the\\ncontent itself. This raises questions about what is actually being learned by the model.\\nA question we will try to answer in this research.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 12, 'page_label': '6'}, page_content='2 Literature Survey\\nAnother crucial factor within this approach is its heavy reliance on long, high-quality\\nreasoning traces. This data needs to be of a sufficient accuracy and well-formed to\\nhelp the model learn effective reasoning. (Yeo, Tong, Niu, et al. 2025; L. Yuan, W. Li,\\nH. Chen, et al. 2024; DeepSeek-AI, D. Guo, D. Yang, et al. 2025) Studies show that\\nmodels can suffer from low-quality reasoning traces, as this may introduce noise and\\nreduce overall performance, especially if the model size itself is relatively small (<3B)\\n(R. Luo, J. Li, C. Huang, and W. Lu 2025). Fortunately, our model size is well over the\\nsuggested limit so it is less likely to be affected with this result. Prior work suggests that\\nfine-tuning should outperform prompting alone (Stiennon, Ouyang, J. Wu, et al. 2020;\\nPerez, Ribeiro, Kiela, et al. 2021; Ouyang, J. Wu, Jiang, et al. 2022), to what extent\\ndoes this holds for our use-case is however, yet to be determined.\\nReasoning with Rewards\\nA rather recent approach to shape model reasoning is using Reinforcement Learning (RL).\\nIn general, such methods provide reward signals based on correctness, explanation quality,\\nor process alignment. The DeepSeek-R1-Zero work (DeepSeek-AI, D. Guo, D. Yang, et al.\\n2025) is a prominent example where pure RL without supervised fine tuning (SFT) is used\\nto incentivise reflective, multi-step reasoning. However, this is far from the first attempt\\nto harness the power of RL to teach models how to reason effectively. Earlier work intro-\\nduced RLHF (Reinforcement Learning from Human Feedback) (Ouyang, J. Wu, Jiang,\\net al. 2022), which used preference-based rewards to improve alignment and response\\nquality. A technique the o1-openAI team is also suspected to be using in the making of\\ntheir recent successful strawberry model, which has demonstrated strong performance on\\ncomplex reasoning tasks (Valmeekam, Marquez, Sreedharan, and Kambhampati 2023;\\nQin, X. Li, H. Zou, et al. 2024). Additionally, more recent studies have shifted toward\\nmore reasoning specific rewards. For example, a study proposes RLEF: Reinforcement\\nLearning through Execution Feedback (Gehring, K. Zheng, Copet, et al. 2025). This\\nmethod uses execution feedback at inference time, to guide LLMs toward correct solutions.\\nA step further into examining how LLMs think, some methods are designed to map out\\nthe model’s entire “thought process” and use reward signals from external verifier to\\nguide the desired reasoning path. This paradigm generally, leverages the Monte Carlo\\nTree Search (MCTS) to its benefit (Qin, X. Li, H. Zou, et al. 2024). A paper built on\\nthis notion that uses process-level reward functions to enable models to learn structured\\nreasoning steps without needing step-by-step supervision. (L. Yuan, W. Li, H. Chen, et al.\\n2024). Another study that applies this also introduces an enhanced iterative preference\\nlearning framework. They break down instance level rewards into fine grained step-wise\\nsignals via MCTS rollouts and apply Direct Preference Optimization (DPO) to update\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 13, 'page_label': '7'}, page_content='2 Literature Survey\\nthe model (Y. Xie, Goyal, W. Zheng, et al. 2024).\\nAnother upcoming way to make use of external verifiers is through Reinforcement\\nLearning with Verified Rewards (RLVR) (Su, Yu, Song, et al. 2025; Yue, Z. Chen, R. Lu,\\net al. 2025; Y. Wang, Q. Yang, Zeng, et al. 2025). In this setup, there is no separate\\ntrained reward model. The signal is directly gathered from an existing deterministic\\nheuristic. This is an approach we also adopt and will discuss in more detail later. It\\nworks well in our case because we are working with classical planning problems, where\\nestablished libraries can be used to evaluate plans reliably. A notable concern with RLVR,\\nas pointed out by existing work, is that while it improves sampling efficiency, it does\\nnot significantly expand underlying reasoning abilities of the base model (Yue, Z. Chen,\\nR. Lu, et al. 2025). Whether we reach the coveted ’AHA’ moment as claimed by the\\nDeepSeek team with this approach, still remains an open question.\\n2.3 Evaluating Reasoning in Language Reasoning Models\\n(LRM)\\nIn previous sections, we briefly touched upon the question of LLMs genuinely reason, or\\nmerely pattern-match against their training data. This section delves deeper into this\\nquestion and explores results seen using different methodologies already proposed in the\\nexisting literature to probe reasoning abilities in both LLMs and their more new and\\nimproved version: Language Reasoning Model (LRM).\\n2.3.1 LLMs: Can they plan?\\nCentral topics of discussion in this paper is to test whether LLMs are capable of planning.\\nAs seen earlier, while LLMs have demonstrated strong performance on a range of tasks,\\nthere is still scepticism on their planning abilities. Some even deem the models to simply\\nbe universal approximate retrieval n-gram models (ref:can llms plan?). This argument\\nclaims that LLMs are not solving tasks with guarantee and rather retrieving approximate\\nguesses for solution. To tackle this scepticism, studies have attempted to put critique\\ninto the training loop. The role of generating this critique, was also assigned to an\\nLLM. In such scenarios where LLMs are encouraged to reason through self-critiquing\\n(Valmeekam, Marquez, and Kambhampati 2023; Weng, H. Guo, H. Liu, et al. 2022) while\\none group claims that the performance improves, another experienced a stark opposite.\\nThis divergence highlights that the effectiveness of self-critiquing, and more broadly, the\\nplanning capabilities of LLMs, can vary significantly depending on the task formulation,\\nmodel architecture, training objective, and evaluation method. Therefore, there have\\nbeen continued efforts made to improve the reasoning abilities of LLMs and consequently\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 14, 'page_label': '8'}, page_content='2 Literature Survey\\nevaluate these improvements (C. Zheng, Z. Liu, E. Xie, et al. 2024; T. Liu, W. Xu,\\nW. Huang, et al. 2025; L. Zhang, B. Wang, Qiu, et al. 2025). The relevant methods have\\nbeen illustrated in detail in the prior section. Our aim through this study is to evaluate\\nthe performance of these paradigms for our problem statement.\\n2.3.2 Rethinking Current Reasoning Benchmarks\\nThe main challenge in evaluating reasoning lies in the ambiguity of the term itself. In tra-\\nditional contexts, reasoning involves: inference, abstraction and generalisation. However,\\nwhen applied to LLMs, reasoning is often assessed through final task accuracy without\\nnecessarily even verifying whether the model followed a coherent reasoning process. As\\na result, models that rely on memorised patterns or dataset artefacts may appear to\\n\"reason\" effectively, even when no actual inferential process is involved. As demonstrated\\nby a literature tackling such evaluation tasks (Du, He, N. Zou, et al. 2023; C. Xie,\\nY. Huang, C. Zhang, et al. 2025; Asai and Fukunaga 2018).\\nThis evaluation strategy is often accompanied with benchmark categories that further\\npromote this naive interpretation of the task. Examples of which were mentioned in the\\nprevious section 2.1. As a consequence there was an emerging need for more reasoning\\noriented benchmarks. Datasets designed that have been designed to test different aspects\\nof reasoning. Multi-hop question answering tasks like HotpotQA (Z. Yang, Qi, S. Zhang,\\net al. 2018) and MuSiQue (Trivedi, Rajani, Xiong, et al. 2022) require models to combine\\ninformation from multiple sources. While tasks like CLUTRR (Sinha, Sodhani, Rajeswar,\\net al. 2019) or SCAN are designed to test systematic generalisation and symbolic reasoning.\\nMore recently, environments such as BabyAI (Chevalier-Boisvert, Bahdanau, Lahlou,\\net al. 2019) and ALFWorld (Shridhar, X. Yuan, Côté, et al. 2020) have been introduced\\nto assess reasoning in interactive, sequential contexts such as planning and control. Our\\ndomain of choice falls very close to the last example in this list. As mentioned earlier in\\nthe introduction section, we use Blocksworld: a classical planning domain dataset. To\\nevaluate model performance in this domain we enact step by step plan generated by the\\nmodel on a simulated environment. Thereby, somewhat circumventing the reliance on\\nsolely the final output. We build on the Blocksworld dataset introduced by Asai (Asai\\nand Fukunaga 2018) , which was intended to bridge symbolic planning and real-world\\nimagery. We then extend it for plan generation tasks by making it more exhaustive. We\\ndo this by: increasing the variety of initial and goal states and adding longer and more\\ncomplex action sequences. This makes the dataset more challenging and better suited to\\nevaluate multi-step reasoning in LLMs.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 15, 'page_label': '9'}, page_content='2 Literature Survey\\n2.3.3 Blocksworld as a Reasoning Benchmark\\nA fellow, recent study uses Blocksworld domain to evaluate reasoning abilities of LRMs\\n(Valmeekam, Marquez, Olmo, et al. 2023). In this study, researchers evaluate an array of\\nmodels ranging from different versions of Claude, OpenAI GPT-4, LLaMA and finally\\nGemini. They benchmark these models across different experimental suites and impor-\\ntantly, consider factors such as efficiency, cost, and reliability. Practical metrics that\\nare often overlooked in most academic research. This group’s findings show that albeit\\nafter a consuming a larger number of inference tokens (Open AI’s o1), LRM is able to\\nexceedingly surpass it’s former LLMs in performance on the simplest task in their portfo-\\nlio. While they have argued in the past that LLMs generate outputs via approximate\\nretrievals (Kambhampati 2024), the authors claim that o1 (an LRM) can supplement a\\nbase LLM with the missing skills required for improved reasoning, without relying on an\\nexternal verifier. However, if one were to look at other practical metrics mentioned earlier:\\nefficiency, cost, and reliability, LRM’s standing becomes rather murky. This model in\\nits form factor at the time, levied an ambiguous costing system wrt inference token\\nsurcharges (the main suspected source of it’s better performance). Additionally, this\\nmodel was not always reliable in its performance with a bit of obfuscation within the same\\ntask and tasks other than that of plan generation within the same domain of Blocksworld.\\nLeading to the authors’ recommendation of using an LLM-modulo approach: an external\\ndomain-specific solver along with an LLM for translation between task language and\\nnatural language. This approach is touted as 100% accurate, reliable and cost-effective.\\nAforementioned paper further holds a notion that has since become a popular opinion\\nwith regards to o1, especially after the recent success of DeepSeek models. The team\\nbelieves that o1 models combine an underlying LLM into an RL-trained system. RL\\nsteers the creation, curation and final selection private Chain-of-Though reasoning traces.\\nThus deeming it fundamentally different in nature from LLMs. Whether, this paradigm\\nis re-producible for our domain within limited constraints is what we explore in our\\nresearch. According to a study RL does not elicit any novel generations leading to an\\n’AHA’ moment as claimed by DeepSeek paper (rlincentivize). This claims that RL\\nprimarily serves to refine and reinforce the behaviours already present in the base model.\\nThereby streamlining the path chosen by the existing base model This suggests that\\nthe perceived gains from RL may stem more from sampling efficiency and response\\nstability than from any genuine growth in reasoning ability. In light of these insights, our\\nwork aims to assess whether RLVR, when applied within a well-structured and symbolic\\ndomain like Blocksworld, can offer genuine improvements in reasoning or merely polish\\nwhat the base model already knows.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 16, 'page_label': '10'}, page_content='2 Literature Survey\\nd\\nb\\nc\\na\\nd\\nb\\nca\\nd\\nb\\nc\\na\\nd\\nb\\nc\\na\\n.\\n.\\n.\\nd c\\na\\nb\\n(unstack c b)\\n(put-down c)\\n(pick-up a)\\n(stack a c)\\n(unstack b d)\\n(stack b a)\\nPlan\\n(:predicates (clear ?block)\\n     (ontable ?block)\\n     (handempty)\\n        (holding ?block)\\n             (on ?above ?below))\\n(: action pick-up\\n:parameters (?block)\\n:precondition (and (clear ?block)\\n(ontable ?block)(handempty))\\n:effect(and (holding ?block) (not (clear\\n?block))\\n(not (ontable ?block)) (not\\n(handempty)) ))\\n(:action stack \\n...\\n(: objects a b c d)\\n(: init (handempty) (ontable a) \\n(ontable    d) \\n(on b  d) (on c  b) (clear a) (clear c)\\n) \\n(:goal  (and    (on b  a) (on  a  c)) )\\nDomainProblem\\ninstance\\nBlocksworld Domain\\nInitial state\\nGoal state\\nFigure 1:PDDL representation of the Blocksworld domain. We start with describing the\\ndomain language itself in the top-left section of the image. Followed-up with an\\nexample problem instance in the box below the domain language description.\\nAfter this is given a sample plan to reach the desired goal state. Enlisted actions\\nin this plan are acted upon in the environment on the right. The environment\\nstate representation has been annotated with initial and goal state.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 17, 'page_label': '11'}, page_content='2 Literature Survey\\n2.4 Limitations and Research Gaps\\nThroughout this chapter we have tried to hint at potential blind spots in the existing\\nmethods of approaches to reasoning with LLMs. In this final section of the literature\\nsurvey, we bring these observations together to present a consolidated view of the current\\ngaps and thereby emphasising our motivation behind the need for this project.\\nIn previous sections we dissected the approaches we intend to take during the course\\nof our research. Out of the methodologies proposed, SFT requires large amounts of\\nreasoning specific data to make any noticeable impact on performance. This dependency\\non data availability highlights the importance of gathering robust data. As eluded to\\nearlier, there have been studies that showed LLMs can tend to pattern match and heavily\\nrely more on structural patterns than on the content of the input data itself. An analysis\\nfirst brought to print in 2022 (M. Li, R. Zhang, X. Chen, and Gupta 2023) and then\\nrevisited as late as the beginning of this year, 2025 (D. Li, Cao, Griggs, et al. 2025). The\\noverall conclusion retained. Even after we can ensure to circumvent this issue through\\nmystifying or obfuscating the data structure, it remains crucial to gather enough of such\\ndata. The fact that AI training tasks require large amounts of data is certainly not\\nunheard of. The novel problem here is that, for purposes of evoking human-like reasoning\\nfrom models, one also needs to find a way to produce high-quality text to train the base\\nmodel on (Ding, Y. Chen, B. Xu, et al. 2023; Y. Wu, Sun, H. Yuan, et al. 2024; L. Luo,\\nY. Liu, R. Liu, et al. 2024; Cui, L. Yuan, Ding, et al. 2024). This is far from trivial.\\nThe current strategies of SFT requires large amounts of high-quality, structured data\\nto train models to generate coherent and interpretable reasoning steps. Crafting such\\ndata, especially in symbolic or multi-step domains is costly, time consuming, and domain\\nspecific. As pointed out in an earlier section, study done to asses the quality of reasoning\\ntraces produced by LLMs continue to show reasoning traces that are often superficial or\\ninconsistent. Whether this idea has changed after the emergence of LRMs like o1 and\\nDeepSeek, is to be evaluated.\\nA crucial limitation from an implementation perspective, which is often overlooked\\nin theoretical discussions, is the need for significant compute and memory resources,\\nespecially for reinforcement learning (RL). RL algorithms are particularly demanding\\nbecause they require large amounts of memory and powerful hardware to run efficiently.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 18, 'page_label': '12'}, page_content='2 Literature Survey\\nAs academic researchers, we have limited access to such resources. This is one of the main\\nreasons why we use Parameter-Efficient Fine-Tuning (PEFT) methods in our research\\n(Hu, Shen, Wallis, et al. 2023). PEFT methods are designed to reduce the computational\\nand memory requirements by fine-tuning only a small subset of model parameters, which\\nmakes them more suitable for environments with limited resources. However, even though\\nPEFT methods claim to not significantly affect model performance, in practice, our\\nresearch speed and sometimes even performance are affected by our hardware limitations.\\nThis challenge is even more noticeable in RL, where memory demands can be especially\\nhigh. To address this, we also use quantization techniques to further reduce the memory\\nfootprint of our models . Despite these strategies, resource constraints remain a practical\\nchallenge throughout our work.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 19, 'page_label': '13'}, page_content='3 Methodology\\n3.1 Introduction\\nIn this chapter we begin by presenting a brief overview of the dataset generation, outlining\\nour motivation behind it, the process we follow and finally the resulting distribution.\\nThis is followed by the model selection process and criterion behind the eventual selec-\\ntion. Since evaluating an LLM response requires checking action correctness and plan\\nvalidity, next we describe how responses are processed. For this, we rely heavily on the\\nunified-planning environment which we will briefly touch upon.\\nThis chapter further elaborates on the methodology we follow in this work across the\\nthree main experimental setups: prompting, Supervised Fine-Tuning, and Reinforcement\\nLearning. The objective is to understand how each of these approaches contributes to\\nreasoning and plan generation in the Blocksworld domain. To this regard, we describe the\\nSFT setup, including pre-processing steps, model input formatting and training details.\\nLastly, we look at the RL setup, more specifically Group Relative Policy Optimization,\\nexplaining how the environment, reward structure, and learning objectives are defined.\\nAdditionally, we also outline the structure and role of different variations of one-shot\\nprompts in producing action plans from language models.\\n3.2 Dataset Generation\\nEach of the following sections builds upon the dataset introduced in its previous section\\nand, together they provide a basis to evaluate the model’s planning ability.\\n3.2.1 Introduction\\nBlocksworld is a legacy environment used to study planning and reasoning. It is a\\nwell-defined domain and yet it contains enough complexity to challenge reasoning models.\\nFor example, through sub-goals that can conflict with one another and tasks that natu-\\nrally break down into smaller sub-problems. Given that the objective of our research\\nis to optimise the reasoning capabilities of the selected model, we choose to generate\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 20, 'page_label': '14'}, page_content='3 Methodology\\na custom dataset within the Blocksworld domain. Usually, problem statements from\\nBlocksworlds domain consist of a set of named blocks that can be stacked on top of one\\nanother or placed on a table. The goal is to move the blocks from an initial configuration\\nto a target configuration using a sequence of valid actions, namely:pick-up, stack,\\nput-down, unstack. Any action can only be performed after required pre-conditions\\nhave been met, upon performing the action the relevant state variables are updated\\naccordingly to reflect the new configuration. Though the rules are simple, solving a\\nBlocksworld task often requires several steps of planning, making it a useful test-bed\\nfor evaluating how well models can reason through intermediate goals to reach a final state.\\nWe are, by no means, the first ones to use Blocksworld domain problems for test-\\ning reasoning abilities of models. The domain has been widely used in classical planning\\nresearch (Fikes and Nilsson 1971), and more recently in learning-based approaches where\\nmodels attempt to generate action sequences or reason through symbolic transitions\\n(Asai and Fukunaga 2018; Y. Zhou, Paduraru, Boteanu, et al. 2020). It has also featured\\nin neurosymbolic settings that aim to integrate neural networks with planning algorithms\\n(Valmeekam, Zhan, Brafman, et al. 2022). Along with program synthesis tasks requiring\\ninterpretable intermediate steps (Lample and Charton 2019). This consistent use across\\ndecades and methodologies makes Blocksworld a reliable benchmark for evaluating rea-\\nsoning models. A more detailed assessment on this topic can be found in the chapter\\npertaining to related work in chapter 2.\\n3.2.2 Blocksworld Dataset Generation\\nAs the blocksworld domain is well-known there are pre-exisitng datasets that have been\\nused for various studies, as mentioned above. This is freely available to use for academic\\npurposes. Although there’s different versions of this dataset available, they are not suffi-\\nciently vast enough for our purposes. We intend to fine-tune LLMs and further proceed\\nwith Reinforcement Learning. Both of these paradigms require an extensive amount of\\ndata points to get ahead of issues such as overfitting and to ensure generalization across\\ndiverse problem configurations. The limited size of available datasets thus would have\\nrestricted the robustness and reliability of our experiments. This emphasizes the need for\\neither synthetic data generation or augmentation methods to support our experimental\\nobjectives.\\nOwing to this insight, we proceed to create our own blocksworld dataset with namely:\\n• 3 bock configurations\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 21, 'page_label': '15'}, page_content='3 Methodology\\n• 4 block configurations\\n• 5 block configurations\\nThus, our generated dataset contains initial and goal configuration pairs of n-number\\nof blocks, wheren ranges from 3...5. Within an nth-block subset our aim is to get an\\nexhaustive set of Blocksworld configurations possible for thatn.\\nStep 1: Generaten blocks\\nStep 2: Create all unique\\npartitions (towers)\\nStep 3: Add permuta-\\ntions within each tower\\nStep 4: Collect all valid\\nconfigurations as states\\nStep 5: Pair all states into ini-\\ntial and goal configurations\\nFigure 2: Flowchart for generating Blocksworld initial–goal state pairs.\\nAlgorithm for init, goal State Pairs\\nLet’s breakdown the steps used in the dataset generation algorithm.\\nStep 1: Generate Blocks\\nWe begin by generating a list ofn unique blocks. These blocks are initially represented\\nas integers from 1...n.\\nStep 2: Create All Unique Partitions (towers)\\nNext, we calculate all possible ways to divide these blocks into one or more tower of blocks\\nor stacks. Each tower is represented as a group of blocks. In this step we recursively\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 22, 'page_label': '16'}, page_content='3 Methodology\\ngenerate all valid partitions, such that no block is repeated. The order of towers is\\nconsidered irrelevant at this point.\\nStep 3: Add All Permutations Within Towers\\nFor towers that contain more than one block, we further generate all permutations of the\\nblock order within that tower. For example: a tower containing blocks: (1, 2) also gives\\nus (2, 1) as a valid variation.1 These permutations represent different ways blocks can\\nbe stacked on top of one another.\\nStep 4: Combine All Valid Configurations\\nWe collect all unique configurations, including original partitions and the new permuta-\\ntions, into a set of complete block arrangements. Each configuration reflects apossible\\nstate in the Blocksworld domain.\\nStep 5: Form All Possible States into Initial–Goal State Pairs\\nFinally, we pair each state configuration with every other state configuration to form\\nexhaustive initial and goal state combinations. We naturally avoid pairing a configuration\\nwith itself. These pairs represent the full set of Blocksworld planning problems that our\\nsystem has used throughout this research.\\nBlock Annotation via Colours Additionally, to name and identify these blocks we\\nascribe colours to them. The idea is to keep the model prompt textual. Each subset ofn\\ncolours was randomly selected from a larger set of 20 available colours. After generating\\na fixed subset ofn colours, we mapped each block number (from 1 to n) to one of the\\nselected colours in order. For example, if the original configuration was (1,2,3) and\\nthe chosen colours for 3-blocks dataset were: violet, teal and brown (which they are)\\nthen the mapping will be: {1: ’violet’, 2:’teal’, 3: ’brown’}. Consequently, the coloured\\nconfiguration of blocks will be: (’violet’,’teal’, ’brown’).\\nProblem Representation\\nAs previously mentioned, in language model training it is generally advisable to maintain\\nNatural Language (NL) flow in input prompts. On these lines, we convert all the tuple\\nblock representation to natural language description followed by other researches when\\nworking with LLMs in the Blocksworld domain.\\nLet’s continue with our previous example configuration of (’violet’,’teal’, ’brown’). This\\ntuple of 3-blocks in natural language representation would look like so:the brown block\\nis clear, the hand is empty, the violet block is on the table, the teal block is on top of the\\nviolet block, the brown block is on top of the teal block.\\n1We represent blocks stacked from bottom to top as arranged from left to right.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 23, 'page_label': '17'}, page_content='3 Methodology\\nBoth the initial and goal states are represented in this natural language format. Each\\ninstance in the dataset is stored as a pair of strings: one for the initial state and one for\\nthe goal state. These are then saved in a structured CSV format. This allows the dataset\\nto be easily parsed and used across different training pipelines.\\nInit Goal\\nFigure 3:Visualisation of example block configuration with initial state (left) to goal\\nstate (right)\\nExample block configuration as represented by natural language\\n“As initial conditions I have that, the brown block is clear, the hand is empty,\\nthe violet block is on the table, the teal block is on top of the violet block,\\nthe brown block is on top of the teal block.\\nMy goal is to have that the violet block is on the table, the teal block is on\\nthe table, the brown block is on the table.”\\nFigure 4: Natural Language representation of example state configuration\\n3.2.3 From Dataset to Prompt\\nHere we describe how structured Blocksworld state pairs are converted into natural lan-\\nguage prompts for LLMs, detailing the instructions, metadata, demonstration formatting,\\nand final prompt template used to support model reasoning.\\nPrompt Design Template\\nNow that we have seen the natural language representation of our problem statement,\\nwe can dive deeper into this and see the structure of the prompt’s design template. We\\ndecide on the template by referring to prompt formats used by fellow researchers working\\non planning and reasoning tasks with LLMs. More specifically, we relied on the prompt\\ntemplate used in the Planbench paper adapting it slightly to suit our task setup.\\nEach prompt follows a consistent template that includes four main parts:\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 24, 'page_label': '18'}, page_content='3 Methodology\\n• a brief introduction to the task\\n• a list of possible actions the model can use\\n• the rules of the Blocksworld environment\\n• the specific problem statement with initial and goal state configurations in NL text\\nOne-shot Prompting\\nWe realise early on during our experimentation that one-shot prompting evidentially\\nperforms better for our problem than zero-shot. Especially in enforcing action compliance.\\nThat is, providing a demonstration helps the model adhere more strictly to the list of\\nallowed actions. So this is the set-up we choose for this research. The prompt template\\nremains the same, with the only difference being an addition of a one-shot example placed\\nright before the actual problem statement.\\nSince it’s one-shot prompting, we also need to include the gold plan2 corresponding to\\nthe demonstration problem. This plan serves as a reference example to guide the model\\nin generating a valid sequence of actions for the actual test input. Following the task\\ninformation and the demonstration, the final part of the prompt is the problem statement.\\nAs shown in the figure below, the problem statement is appended with the starting\\n[PLAN] tag to indicate where the model should begin generating the output plan.\\nOne-shot Prompt for Plan Generation\\nI am playing with a set of blocks where I need to arrange the blocks into\\nstacks. Here are the actions I can do\\n↪→ Pick up a block\\n↪→ Unstack a block from on top of another block\\n↪→ Put down a block\\n↪→ Stack a block on top of another block\\nI have the following restrictions on my actions:\\nI can only pick up or unstack one block at a time.\\nI can only pick up or unstack a block if my hand is empty.\\nI can only pick up a block if the block is on the table and the block is\\n↪→ clear.\\nA block is clear if the block has no other blocks on top of it and\\n↪→ if the block is not picked up.\\nI can only unstack a block from on top of another block if the block I am\\n↪→ unstacking was really on top of the other block.\\nI can only unstack a block from on top of another block if the block I am\\n2Generation of the gold plan is a separate topic discussed in more detail in the upcoming sections.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 25, 'page_label': '19'}, page_content='3 Methodology\\n↪→ unstacking is clear.\\nOnce I pick up or unstack a block, I am holding the block.\\nI can only put down a block that I am holding.\\nI can only stack a block on top of another block if I am holding the block\\n↪→ being stacked.\\nI can only stack a block on top of another block if the block onto which I\\n↪→ am stacking the block is clear.\\nOnce I put down or stack a block, my hand becomes empty.\\nOnce you stack a block on top of a second block, the second block is no ↪→\\nlonger clear.\\n[STATEMENT]\\nAs initial conditions I have that, the green block is clear, the hand is\\n↪→ empty, the red block is on the table, the pink block is on top of the\\n↪→ red block, the green block is on top of the pink block.\\nMy goal is to have that the green block is on the table, the red block is\\n↪→ on top of the green block, the pink block is on top of the red block.\\nMy plan is as follows:\\n[PLAN]\\nunstack the green block from on top of the pink block\\nput down the green block\\nunstack the pink block from on top of the red block\\nput down the pink block\\npick up the red block\\nstack the red block on top of the green block\\npick up the pink block\\nstack the pink block on top of the red block\\n[PLAN END]\\n[STATEMENT]\\nAs initial conditions I have that, the brown block is clear, the hand is\\n↪→ empty, the violet block is on the table, the teal block is on top of\\n↪→the violet block, the brown block is on top of the teal block.\\nMy goal is to have that the violet block is on the table, the teal block\\n↪→ is on the table, the brown block is on the table.\\nMy plan is as follows:\\n[PLAN]\\nFigure 5: One-shot prompt template used for plan generation\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 26, 'page_label': '20'}, page_content='3 Methodology\\n3.2.4 Dataset splits\\nThis section explains how the dataset is split into training, validation, test, and contami-\\nnated sets. We decide on ratios that ensure diverse training and effective evaluation.\\nSplit Overview\\nAs is the norm in AI projects, we split our dataset into train, validation and test sets\\nacross our three different block-configuration sizes: 3, 4, and 5 blocks. As we saw earlier in\\nsubsection 3.2.2, within each block size, the number of states and total plan combinations\\nare calculated using all pairwise combinations of distinct initial and goal states. This\\nresults in a near-exhaustive set of possible planning problems for each configuration size.\\nThe train, validation, and test splits follow a 70-10-20 ratio. This ensures that the model\\nhas access to a wide variety of examples during training while still being evaluated on\\nunseen (and sometimes seen) scenarios during validation and testing. The details of the\\nsplits are as follows:\\n• 5-blocks: The 5-block configuration produces 436 unique states, resulting in\\n436 ×435 = 189,660 problem statements. This is then split into:\\n– Training set: 132,762 problems\\n– Validation set: 18,966 problems\\n– Test set: 37,932 problems\\n• 4-blocks: The 4-block configuration produces 70 unique states, resulting in70 ×\\n69 = 4,830 problem statements. This is then split into:\\n– Training set: 3,381 problems\\n– Validation set: 483 problems\\n– Test set: 966 problems\\n• 3-blocks: The 3-block configuration produces 13 unique states, resulting in13 ×\\n12 = 156 problem statements. This is then split into:\\n– Training set: 110 problems\\n– Validation set: 15 problems\\n– Test set: 31 problems\\nContaminated Test Set As eluded to earlier, there is a slight adjustment made to\\nthe test set in these splits. More specifically, the test set is composed of two equal parts:\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 27, 'page_label': '21'}, page_content='3 Methodology\\n• 10% of the total data points form theclean test set\\n• 10% of the total data points form thecontaminated test set.\\nThis contaminated test set (i.e 10% of total examples) is included in the training data\\nto simply observe whether performance improves on data the model has already seen\\nduring training - hence, the name. This design is to evaluate the model performance on\\nexamples it has seen during training versus entirely unseen ones.\\nSince we conduct one-shot prompting experiments, our effective training data\\nsize is halved to avoid any contamination between the demonstration and the actual\\nproblem statement within a single prompt. For the validation and test sets, we use\\nexamples from the training data to construct demonstrations. This is a deliberate move.\\nPrior work suggests that the demonstration mostly helps the model learn the structure\\nof action sequences, rather than contributing directly to planning ability. Moreover,\\nsince the model had already seen these examples during training, reusing them as\\ndemonstrations poses no risk of data leakage or unfair advantage.\\nFigure 6: Overall % of dataset composition based on block size\\nDataset Split Generation\\nIn order to create our dataset splits, we first divide the set of unique problems into\\ntraining, validation, and test sets according to the above mentioned percentages of\\n70-10-20. These splits are replicated across all block-set sizes.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 28, 'page_label': '22'}, page_content='3 Methodology\\nFigure 7: Train, validation, and test distribution for 3- and 4-block problems\\nFor the training and validation sets, each unique problem is instantiated by randomly\\nselecting one out of two colour sets. For example, a configuration involving blocks (1,2,3)\\nhas two possible mappings. The first maps numbered blocks to 1: ’violet’, 2: ’teal’,\\n3: ’brown’, and the second maps them to 1: ’pink’, 2: ’green’, 3: ’red’. At the time\\nof instantiation, the system randomly selects one of these two colour-maps. So the\\nblock configuration after colouring might look like (pink,green,red) or (violet,teal,brown)\\ndepending on the selection.\\nFor the test set, we follow a slightly different strategy. Firstly, the test set is\\nrandomly split into two, pure test set and contaminated test set. For the test set, we\\nfollow a split-in-two strategy to create both clean and contaminated test subsets. We\\nthen begin colouring the configurations like so:\\n• Clean test set (10%):Each problem is instantiated using a single colour set.\\nThis problem is used only in the test set. These examples remain completely unseen\\nby the model during training.\\n• Contaminated test set (10%):Each problem is instantiated twice using two\\ndifferent colour sets. One version is added to the training set, while the other is used\\nas a test example. This allows us to analyse whether the model performs differently\\non problems it has already seen (albeit in different colours) during training versus\\nthe ones it has not.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 29, 'page_label': '23'}, page_content='3 Methodology\\nThis approach ensures that half of the test set consists of completely unseen exam-\\nples (clean), while the other half includes problems the model has been exposed to\\n(contaminated), but under a different colour configuration.\\nScaling Effect with Increasing Block Count\\nAs shown in the earlier section of Split Overview 3.2.4 and further elaborated on in\\nfigure 7 the data distribution across different block size subsets is highly skewed. While\\nincreasing the number of blocks:n, leads to a combinatorial growth in the number of\\npossible states. Higher values ofn also introduces significantly more complexity. More\\nspecifically a higher value ofn tend to require longer and more intricate plans, as the\\ngoal states become more randomized and are often much further from the initial state in\\naction steps. As explained in the upcoming sections, to avoid inundating the model at\\ntimes the scope is limited to configurations with only 3 and 4 blocks.\\n3.3 Model Selection\\n3.3.1 Selection Criterion\\nWe work with limited computational resources, including memory and training power,\\nwe have to confine ourselves to relatively smaller models (more specifically, in the less\\nthan 9B parameters range). Furthermore, since our task is that of multi-step planning it\\nis crucial that the model is instruction fine-tuned and capable of following structured\\nprompts. This combination narrows down our search to a select few open-source instruct\\nmodels that are widely used. The following subsections describe the shortlisted models\\nand the rationale behind selecting them for our experiments. But before we move to that,\\nwe need to take a small detour and look at how we extract coherent action sequences\\nfrom LLM text and which techniques are used to parse, clean, and validate the generated\\nplans for further evaluation.\\n3.3.2 Mathematical Representation of Planning Problems\\nIn classical planning, a problem is commonly described by the tupleP = ⟨D, I, G⟩, where\\nD is the domain,I is the initial state, andG is the goal. The domain itself is defined\\nas D= ⟨F, A⟩, whereF is the set of fluents—essentially predicates used to describe the\\nworld—and A is the set of possible actions. The state space of the problem is defined by\\nall possible truth assignments over the fluents.\\nEach action is defined by its name, a set of variables, preconditions, and effects. The\\npreconditions determine when the action is allowed (i.e., what must already be true in the\\ncurrent state), and the effects specify how the world changes after the action is applied.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 30, 'page_label': '24'}, page_content='3 Methodology\\nThe effects are further split into anadd list (predicates made true) and adelete list\\n(predicates made false). When the variables in an action are replaced with specific objects\\nfrom the domain, the action is consideredgrounded; otherwise, it remains in its more\\nabstract, lifted form.\\nFor example, in the Blocksworld domain, the action of stacking a block on top of another\\nblock is written in our implementation of the framework as follows:\\n(:action stack\\n:parameters (?below ?above)\\n:precondition (and (clear ?below) (holding ?above))\\n:effect (and (hand-empty) (clear ?above) (on ?above ?below)\\n(not (clear ?below)) (not (holding ?above))))\\nListing 3.1: Sample unified-planning implemented for blocksworld action forstack\\nIn the above snippet of thestack action, the parameters define the objects being acted\\non—in this case, the block to be stacked and the block it will be placed upon. As\\nmentioned earlier and further illustrated in the example, predicates (or fluents) are\\ndefined by their assigned Boolean values. In our case, there are five fluents:on, clear,\\non_table, holding, andarm_empty. In the snippet, first the preconditions state that\\nthe hand must be holding the above block in order to stack it on top of the below one\\n(i.e., the predicate(holding ?above) is true). The effect specifies that after this action\\nis executed the arm will be empty. The above block will now be successfully on top of the\\nbelow block, which will no longer be clear. The arm will however be empty and thereby\\nmaking the above block clear.\\nLikewise, all other actions namely: unstack, pick-up, put-down are defined in the\\nframework to build executable plan objects. We will see in the coming subsections how\\none can link LLM responses to these well-defined actions. Following which we build\\nsimulation objects to evaluate validity of model generated action plans. Furthermore, we\\nalso avail the solver functionality of our framework to ground our problems and estimate\\noptimal plans.\\nAn Overview of our framework: unified-planning Library\\nThe unified-planning library is a Python framework that allows us to model, execute\\nand even solve classical planning problems in a standard way. As established above, we\\ndefine the Blocksworld domain within this framework using PDDL i.e fluents, actions\\nand domain objects. This enables the library to provide us with technical support to\\nsimulate and solve our problem statements. We will further see how we use it in this\\nproject to interpret, validate, and simulate the plans produced by the language model.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 31, 'page_label': '25'}, page_content='3 Methodology\\n3.3.3 Translating Model Output into Executable Plans\\nAs seen earlier in Figure 3.2.3, we use this general prompt template. Once the model\\nis prompted, using regex simply first extract the plan text that appears between the\\n[PLAN] and [PLAN END] tags.\\nNext, we split the extracted plan into individual action-sequences using newline\\ncharacters as separators. Each action sequence is then converted into an action tuple.\\nBy removing unnecessary words we simplify the phrasing to create a stripped down\\nversion. Each tuple takes the form:(action, object_1, object_2), whereobject_2\\nis optional. This is because some actions, such asput-down or pick-up, only act on a\\nsingle object.\\nAfter multiple rounds of experimentation with LLMs we realise that our expectations\\naround the phrasing of action-sequences need to be less rigid. Improving with iterations,\\nthe model is now not required to follow the exact wording from the pre-defined action\\nlist, as long as the core meaning is preserved and the sequence remains logical. For\\ninstance, instead of requiring the full phrase “unstack the block1-name block from on top\\nof the block2-name block,” we also accept shorter versions such as “unstack block1-name\\nfrom block2-name.”\\n3.3.4 Building Executable Plans from Actions\\nNow that we have our action tuples, we can move on to building executable plan objects\\nfrom them. Each tuple is passed through a mapping function that links it to a valid\\naction in our implementation of the Blocksworld domain using theunified-planning\\nlibrary.\\nOnce the individual action instances are generated, we use this list of actions to create a\\nunified plan object. This plan object is then used to compute the relevant evaluation\\nmetrics required for assessing plan quality. This will be elaborated on further in chapter\\n4.\\nNow that we have established how model output responses are interpreted and verified\\nwithin our framework, we can return to the core question of model selection. With\\nthis foundation in place, we are better equipped to assess which models are suited for\\nhigh-quality generations, off course within our limitations.\\n3.3.5 Benchmarking Candidate Models\\nThis section explores the process of selecting a suitable language model for plan\\ngeneration. Several viable instruction-tuned models are tested based on output quality,\\nplanning coherence, and resource efficiency. We describe the models considered, the\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 32, 'page_label': '26'}, page_content='3 Methodology\\nevaluation strategy followed, and the final choice made.\\nAs discussed earlier, all candidate models are open-source and fall within the\\n7–9B parameter range to ensure practical inference times and manageable memory\\nrequirements. We evaluate each candidate on its ability to produce structured\\nsyntactically and semantically correct action sequences. The models considered\\ninclude: Qwen2-7B-Instruct, Qwen2.5-7B-Instruct, google/Gemma-1.1-7B-it and\\ngoogle/Gemma-2-9B-it, microsoft/phi-4-multimodal-instruct, Mistral-7B-Instruct-v0.1\\nand Mistral-7B-Instruct-v0.2.\\nEvaluation Metrics and dataset\\nTo evaluate the model output quality, we define the following metrics. Some of these\\nmetrics are reused across other experimental set-ups as well, so it is crucial to introduce\\nthem clearly at this stage.\\n• Attempts: The number of attempts required by the model to produce a parsable\\nand hence, format-compliant response. This reflects the model’s consistency and\\nreliability in adhering to the expected output structure.\\n• Diff in plan length:The absolute difference in the number of steps between the\\nmodel-generated plan and the corresponding gold (ground-truth) plan. This is like\\na distance metric used to check for optimality conditions.\\n• Terminate: Abooleanvaluecheckingwhethergeneratedplaneventuallyterminates\\nto the goal state. A terminating plan indicates correct sequencing of actions that\\nsatisfy all goal conditions.\\n• Action-steps away from goal:An integer value indicating additional action\\nsteps required to reach the goal state from the current state of the environment.\\nThis gives a decent estimate of how far off the generated plan is from achieving the\\ndesired goal, with lower values indicating better performance.\\n• Action-steps in gold plan:The total number of steps in the ground-truth (gold)\\nplan. This serves as a baseline for comparing the length and efficiency of the\\nmodel-generated plan. Furthermore, this value provides us with an estimate of the\\ncomplexity of the problem statement itself.\\nWe construct experiments with initially, the basic version of the example prompt 3.2.3\\nand later add an additional helper string to it. We keep the temperature value fixed\\nto 0.0 to enforce reproducibility. For this evaluation we use a 15 sample big subset of\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 33, 'page_label': '27'}, page_content='3 Methodology\\n3-block train dataset, as it is smaller in size while still posing a reasonable challenge for\\nthe models to handle.\\nModel Setup Mode\\n(Attempts)\\nMean\\n(Diff in\\nPlan\\nLength)\\nMean\\n(Terminate)\\nMedian\\n(Steps from\\nGoal)\\nMode\\n(Gold Plan\\nSteps)\\nMistral-7B-Instruct-v0.3 3 — 0.00 4 4\\nMistral-7B-Instruct-v0.1 3 — 0.00 4 4\\nmicrosoft/phi-4-\\nmultimodal\\n1 3.00 0.07 4 4\\nalibaba/QWEN-2.5-7b 1 0.50 0.13 4 4\\nalibaba/QWEN-2-7b 1 0.50 0.13 4 4\\ngoogle/gemma-2-9b 1 0.00 0.27 4 4\\ngoogle/gemma-1.1-7b 1 0.00 0.27 4 4\\nTable 3.1: Aggregated evaluation metrics for different candidate models\\nFindings from the Evaluation metrics\\nIt is noteworthy that neither Mistral models are able to produce any coherent plans let\\nalone plans that terminate even after mostly trying until the max attempts to query the\\nmodel, i.e 3 queries per problem statement. For these models, the other columns such as\\nterminate and steps from goal are not very meaningful, as they only offer insights once a\\ncomprehensible plan is available, which is entirely missing in this case.\\nIn contrast, Microsoft’s phi-4 model typically uses only a single attempt to generate\\ncoherent plans. While this plan does reach the goal state, it is not very optimal. However,\\nthis is balanced by the median (steps from goal) value of 4, which is a strong indicator\\nof potential. This shows us that even though most model produced plans here do not\\nterminate, they were only a few steps away from termination. Hence proving its metal in\\nproducing long traces of valid action plans.\\nFollowing this, both QWEN models exhibit similar performance across both sizes. Even\\nthough they are able to produce coherent plans mostly within a single attempt, they are\\nsub-optimal and very few in number.\\nFinally, the google/gemma models perform the best. They have the highest terminate\\nrate and all of the plans are within the optimal length. This is a strong sign of their\\npromise as suitable model choice. On top of that, the low median (steps from goal) value\\nfurther supports their reliability.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 34, 'page_label': '28'}, page_content='3 Methodology\\nWith these initial findings, we can clearly see potential in the QWEN, Gemma models\\nand to some extent even phi-4 showing some potential. After minor prompt refinements3\\nand a final round of evaluations, we initially select google/gemma-2-9b-it as our preferred\\nmodel. However, early into our experimentation with prompting we discover that our\\nAPI call to the gemma-2-9b are being redirected to gemma-3-12b-it. Luckily, since\\ngemma-3-12b-it remaines within our hardware constraints and continued to perform\\nwell, we proceed with it as our final model choice.\\n3.4 Prompting Setup\\n3.4.1 Chain-of-Thought Prompting\\nWe experiment with seven different prompt incantation to encourage the model\\ngeneration of Chain of Thought reasoning trace. The first string is kept as the most\\nwidely used CoT incantation from existing literature. Building upon this, we design the\\nother variants tailored more specifically to our planning-based use case. We also adapt\\nthe original CoT incantation proposed in the seminal CoT paper to better align with our\\ntask requirements. Additionally, we explore a delayed reasoning approach inspired by\\na recent study discussed in the Related Work 2 section, where the model is forced to\\n\"wait\" before generating a plan, to evaluate whether such a simple method could perhaps,\\nenhance reasoning quality.\\nWe append each CoT incantation string to our base one-shot prompt example 3.2.3 along\\nwith the helper string. Then evaluate this on the 3-block validation dataset containing\\n15 problems. This dataset is ideal for running small-scale preliminary experiments. We\\ncompare the number of problems solved successfully by our chosen model with and\\nwithout the incantation. This change in correctness rate helps us quantify the extent\\nto which the incantation truly makes a difference. The seven CoT incantations used for\\ninitial experiments on the small batch are listed below, along with the respective change\\nin correctness rate they bring as compared to baseline results.\\n3We append a helper string to promote format compliance: \"Answer within [PLAN] [PLAN END] tags\".\\nWith this minor adjustment our chosen model successfully produces almost all generations according\\nto the expected format.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 35, 'page_label': '29'}, page_content='3 Methodology\\nS. No. Incantation Change in\\nRate\\n1. Let’s think step by step 0\\n2. Step by step reason about the actions you choose to take+2\\n3. Step by step reason about actions you play. 0\\n4. Step by step reason about actions you take. -1\\n5. Strategise at each step before you choose an action, then\\nform a final plan.\\n-4\\n6. Wait and then -6\\n7. Wait, reason step-by-step about the actions you choose to take. Only\\nanswer within ‘[PLAN] [PLAN END]’ tags.\\n+2\\nTable 3.2:Impact of CoT Incantations on Plan Correctness Rate Relative to Baseline\\n(No CoT Prompt)\\nFollowing our preliminary experiments, we want to further evaluate how the performance\\ntrend scales on larger datasets. Specifically, we focus on two CoT incantations whose\\nresults were particularly noteworthy: one that showed a clear performance improvement\\n(hereafter called the : “good” incantation). The other that intuitively seemed promising\\nbut performed poorly in practice (hereafter called the: \"not-so-good\" incantation). Both\\nof these have been highlighted in the table above. Our goal is to investigate how these\\neffects change with scale—whether the improvements hold, diminish, or become more\\nnoticeable. We also want to understand whether the observed gains can be truly ascribed\\nto the quality of the incantation or if any added prompt string could yield similar results.\\nTo this end, we compare the change in correctness metric again, this time for both\\nincantations across datasets.\\nFor this evaluation we use both subsets from both 3 and 4 block configurations. Our aim\\nis to evaluate on a large enough dataset for our purposes and yet not highly complex like\\nthe 5-block configuration. Additionally, since this setup uses inference-only generation,\\nthere is no risk of data contamination. Similar to the previous experiment, the CoT\\nincantation’s result is then compared again to the baseline for the respective data subset.\\nFollowing this comparison, we analyse how the improvements behave with scale.\\nThe table illustrates the percentage of correctly terminating plans (i.e., plans that\\nsuccessfully reached the goal state). The values are computed under three conditions:\\nbaseline (no incantation), with the effective CoT incantation, and with the not-so-good\\none. We observe that across all splits, both incantations improve plan correctness\\nsignificantly over the baseline. So in general, we note that it is better to include some\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 36, 'page_label': '30'}, page_content='3 Methodology\\nSplit,\\nn = Number\\nof blocks\\nBaseline (no CoT) \"Good\" incantation Not-so-good incantation\\nVal, n = 3 7/15 = 0.46 8/15 = 0.53 6/15 = 0.40\\nTest, n = 3 10/31 = 0.32 13/31 = 0.42 16/31 = 0.52\\nVal, n = 4 56/483 = 0.12 134/483 = 0.28 153/483 = 0.32\\nTest, n = 4 95/966 = 0.098 262/966 = 0.27 298/966 = 0.31\\nTable 3.3: Effect of CoT incantations on plan correctness with Scale\\nform of guided reasoning rather than none at all. Even though the two strings perform\\nquite differently, both manage to shift the model’s output closer to valid plans.\\nHowever, the extent of this improvement is strongly influenced by the scale of the task.\\nWhile in the 3-block configuration the effective string outperforms the weaker one by 10%.\\nAs the complexity increases and the data size increases i.e in the 4-block configuration we\\ncan see that the performance of both incantations begins to converge, with a relatively\\nminute difference in correctness percentages. This suggests that with sufficient task\\ncomplexity and scale, even less effective prompts may benefit from guided reasoning.\\n3.5 Supervised Fine-Tuning (SFT)\\nFollowing the prompting setup, we now describe our Supervised Fine-Tuning pipeline,\\nwhich adapts the model to structured reasoning through example-based learning. As\\ndiscussed in Chapter 2, SFT is a widely used approach for adapting large language\\nmodels to downstream tasks. SFT has been studied to enable LLM to internalise\\ntask-specific patterns beyond what is possible through prompting alone. In our context,\\nthis ability plays a crucial role in reinforcing reasoning strategies.\\nIn the following sections, we detail the preparation of input data for SFT, along with the\\nmodel architecture, training and evaluation setups.\\n3.5.1 Overview of SFT\\nSFT is a distinctly different approach to planning with llms than the previously discussed\\nsetup. In this paradigm, we hope to elicit reasoning from models through a much more\\ndirect exposure to expected output. SFT helps the model learn how to reason by training\\nit on examples of correct behaviour. While prompting gives the model instructions at\\ntest time, SFT goes a step further by showing the model many examples during training,\\nin the hopes that it learns patterns more deeply. This is especially useful for tasks that\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 37, 'page_label': '31'}, page_content='3 Methodology\\nrequire careful, step-by-step thinking, like planning. Expectation is that with enough\\ntraining, model will eventually start to pick up the logic and structure behind good\\nanswers.\\nGoing into this task, this aptly sums up our motivation. We aim to reinforce\\nthe reasoning process by letting the model see well-structured examples of input and\\noutput pairs. As illustrated in the previous section describing data generation 3.2.3,\\nwith the aid ofunified-planning library we are able to produce ground truth, also\\nreferred to as gold plans. These gold plans act as expected outputs in our SFT setup.\\nIn doing so, we hope the model begins to realise useful strategies. Strategies such as,\\nhow to break down a goal into smaller steps, or how to choose valid actions. Like any\\nsupervised learning tasks, these learned tactics can be applied at test time, when solving\\nnew problems.\\n3.5.2 Preparation of Input Data\\nPre-processing Data\\nAn earlier section has already detailed the process of transforming our generated\\nthe dataset into input prompts, culminating in the final prompt template shown in\\nFigure 3.2.3. For SFT purposes, we further process this prompt to fit the format\\nexpected by the chosen model during training. This includes pairing each prompt with\\nits corresponding ground truth output thus ensuring the input-output pairs follow the\\ncausal language modelling format. Furthermore we add special tokens if required by the\\ntokenizer, and finally, tokenize this resulting data file.\\nWe use a chat template to format our inputs before giving them to the model.\\nThis template follows a structure similar to how users and assistants interact in a chat.\\nIt wraps the prompt in special tokens like<|user|> and <|assistant|>, which help\\nthe model understand who is speaking and what kind of response is expected.\\nUsing a chat template is especially helpful for instruction fine-tuned models like ours.\\nThese models have been trained on large datasets of user-assistant conversations. As a\\nresult, they are known to perform better when inputs follow a familiar chat format. By\\nsticking to this style, we want to make it easier for the model to follow instructions and\\ngenerate responses that align with our planning task.\\nIn our task,causal language modellingis the training method we use during SFT.\\nThis technique works by teaching the model to predict the next word (or token) in a\\nsequence, one step at a time, from left to right. This fits well with our planning task,\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 38, 'page_label': '32'}, page_content='3 Methodology\\nCausal LanguageModelling\\nAll\\nbest\\nMASK\\nthe\\nvery\\nInputtokens Outputtoken\\nFigure 8:This figure illustrates causal language modelling, where the model predicts the\\nnext token in a sequence using all previous tokens as input.\\nwhere each action depends on the ones before it. On this regard, after pairing our\\none-shot prompt with its respective ground truth plan, we append this expected outcome\\nto get the model input. For the expected outcome, also referred to as labels, we simply\\nduplicate the input however, this time the input tokens are masked. Masking ensures\\nthat the model does not see those tokens at training time. This is a standard practice in\\ncausal language modelling tasks like ours. This practice is used to make sure the model\\nfocuses on learning how to generate the correct continuation, rather than unnecessarily\\nreproducing tokens from the input.\\nTokenisation Pipeline\\nOnce the input-output pairs are formatted using the chat template, we convert them\\ninto token IDs using the Gemma-3 model’s tokenizer provided by hugging-face.4 This\\nstep transforms our textual data into a numerical format that the model can understand\\nand process. Tokenisation is done in a way that preserves the structure of the prompt,\\nincluding special tokens like eos, <|user|> and <|assistant|> etc. Even though\\nit is not a concern for our project due to the model choice and dataset being used,\\ntokenisation also ensures the final sequence stays within the model’s context length.\\nTo better understand the structure and complexity of our dataset, we will now\\nanalyse the distribution of input token lengths across the 3, 4, and 5-block configurations.\\nThis helps verify that the inputs fall within the model’s context window. Furthermore,\\nhelp us make decisions around batch size and training stability. Longer prompts\\nintuitively reflect more complex planning scenarios. This may increase the reasoning\\nburden on the model.\\n4Please note, at train time we also need to pass a task specific custom data-collator object from\\nHuggingFace to the model trainer class to enable causal language modelling at train time.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 39, 'page_label': '33'}, page_content='3 Methodology\\n3 blocks 4 blocks 5 blocks\\nNumber of Blocks in the Configuration\\n600\\n700\\n800\\n900\\n1000Number of T okens\\nT oken Length Distribution Across Block Configurations\\nFigure 9: Token length distribution across block-set configuration\\nThe following figure ( 3.5.2) shows how the number of input tokens increases with the\\nnumber of blocks in the planning problem. Each box plot represents a different block\\nconfiguration: 3, 4, 5 blocks and the respective distribution of token lengths for each. As\\nexpected, problems with more blocks result in longer input prompts. This is because\\nmore blocks introduce additional objects, relations, and actions, which naturally lead to\\nlonger state descriptions and plans.\\nThe token count gradually increases from around 580 tokens for 3 blocks, to\\n650–750 for 4 blocks, and further to 850–1000+ for 5 blocks. The variability also\\ngrows with complexity, especially in the 5-block case. Since we use causal language\\nmodelling for SFT, input and output sequences must be aligned in length. Based on this\\ndistribution, we set a padding threshold of 1200 tokens for all inputs. We use the default\\npad token with id 0. This is further explicitly added as an ignore index at training\\ntime. This comfortably covers the longest examples in our dataset, even for the 5-block\\nconfiguration. Moreover, our chosen model,gemma-3b-12b-it, supports long context\\nwindows, so this sequence length remains well within its capabilities.\\n3.5.3 SFT Training Implementation\\nWe implement SFT using the Hugging Face transformers and trl libraries. As established\\nin the section about Model Selection (section 3.3) the model we will be working with\\nfrom this point onward isgemma-3b-12b-it from Hugging Face. The training objective\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 40, 'page_label': '34'}, page_content='3 Methodology\\nfollows causal language modelling, where the model is trained to predict the next token\\nin the sequence given all previous tokens. Experimentation is done on NVIDIA A40\\nGPUs, which come with a few constraints in terms of memory capacity and compute\\npower. Due to this consideration, we decide to use a Parameter-Efficient Fine-Tuning\\n(PEFT) technique: Low-Rank Adaptation (LoRA) for our train implementation.\\nLow-Rank Adaptation\\nLoRA is a technique that helps reduce the number of trainable parameters and makes\\ntraining more efficient without needing to update the full model. In the attention layers,\\nthis PEFT method takes a large matrix and decomposes it into two smaller low-rank\\nmatrices (3.5.3), which results in a drastic reduction of several parameters, which needs\\nfine-tuning. This making it feasible to tackle large models even with conservative hardware\\nsetups. In their paper, the LoRA authors also claim that this method achieves comparable\\naccuracy to full fine-tuning across various tasks. Whether this stands true for our project,\\nis yet to be tested. A deeper understanding of LoRA can be developed in the literature\\nsurvey chapter 2.\\nFigure 10: Representation of LoRA, where only A and B matrix weights are trained.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 41, 'page_label': '35'}, page_content='3 Methodology\\nUnderstanding Loss Computation in SFT\\nDuring training, the model is given an input and an expected output. It tries to predict\\nthe next word in the output, one word at a time. To achieve this, the expected output\\nis shifted by one position so the model can learn to guess the next word correctly. The\\nmodel’s guesses (called logits) are compared with the actual next words using the cross-\\nentropy loss function. This shows how far off the model was. The model then updates its\\nweights and biases to reduce this error and improve its predictions for the next iteration.\\nFollowing is the algorithm’s mathematical representation:\\nLSFT = −\\nT∑\\nt=1\\nlog Pθ(yt |x, y<t) (3.1)\\nThis equation represents the standard loss used during Supervised Fine-Tuning.\\nThe model, with parameters θ, is trained to generate the correct output sequence\\ny = (y1, y2, ...,yT) given an input promptx. At each timestept, the model predicts the\\nnext tokenyt conditioned on the input and all previously generated tokensy<t. The\\ntraining process adjusts the model’s weights to maximise the likelihood of the correct\\noutput sequence across all training examples.\\nIn our case, the input x consists of a long prompt containing the Blocksworld\\nstate, goals, and reasoning instructions, while y is the desired response, such as a\\nChain-of-Thought trace or plan. This formulation allows the model to learn fine-grained\\npatterns over long sequences and helps it internalise the structure of valid reasoning.\\nThe exact values for key hyperparameters like the number of training epochs, batch\\nsize, learning rate, gradient accumulation steps, weight decay etc. will be discussed in\\nthe following chapter. Choices are made to balance performance and memory usage,\\nespecially considering that we train on relatively long sequences. To reduce memory\\nload, we enable gradient checkpointing and use LoRA for PEFT. This allows us to train\\nlarge models like gemma-3b-12b-it even on limited hardware.\\nWhile this section covers the base setup, and gives an understanding of the\\nworkings of SFT; further specifications with regard to experimentations are discussed in\\nthe Experimentation and Results chapter 4.\\n3.6 Reinforcement learning with GRPO\\nWe have now established an understanding of two major setups. Next attempt at eliciting\\nreasoning from LLMs involves stepping into the paradigm of Reinforcement Learning. In\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 42, 'page_label': '36'}, page_content='3 Methodology\\nthe literature survey (chapter 2) we discuss multiple studies that demonstrate the use\\nof reinforcement learning with language models. As discussed in the previous chapter,\\nthis tactic has gained traction in the language modelling space primarily through the\\nReinforcement Learning from Human Feedback (RLHF) framework. In this framework, a\\nreward model trained on human preferences guides the policy optimisation of a language\\nmodel. This approach was notably used in training InstructGPT (Ouyang, J. Wu, Jiang,\\net al. 2022) and later in ChatGPT, leading to significant improvements in alignment\\nand helpfulness. More recent work has explored strategies besides RLHF for reasoning\\nand planning. Strategies such as: Reinforcement Learning with AI Feedback (RLAIF),\\nReinforcement Learning with Verified Rewards (RLVR) and Agentic learning.\\nThese efforts mark a clear shift from purely imitation based fine tuning towards\\niterative interaction and feedback, allowing models to refine their behaviour based on\\nlong-term objectives. In our work, we adopt the Reinforcement Learning with Verified\\nRewards approach, specifically using the method recently published by the DeepSeek\\nteam (DeepSeek-AI, D. Guo, D. Yang, et al. 2025) and promoted by fellow researchers\\nfor tasks like ours, GRPO. This approach offers academic research a novel opportunity\\nto enhance the abilities of smaller models. Leveraging our choice of a classical planning\\ndataset Blocksworld to our advantage we harness the capabilities ofunified-planning\\nlibrary to simulate state-wise game conditions and generate granular, tangible reward\\nsignals for training our system.\\nIn the upcoming sections we discuss GRPO’s algorithm, input data preparation\\nand finally, designing a reward signal. Additionally we further elaborate our training and\\nevaluation setups.\\n3.6.1 Introduction to GRPO\\nGuided Reinforcement with Policy Optimisation (GRPO) is a method for improving\\nlanguage models using reinforcement learning. Instead of relying solely on human\\nfeedback, it guides the model with clear, verifiable rewards that check if the output\\nmeets the desired format and/or solves the task correctly. This makes it easier to train\\nmodels for reasoning and planning without needing large amounts of human-labelled data.\\nTraditional methods like RLHF rely heavily on human feedback to guide the\\nmodel towards desired behaviour. While this is proven to be effective for alignment, it is\\nan expensive and slow approach. In the literature survey chapter we also decode how\\nthis method often struggles with tasks that need precise reasoning or planning, because\\nhumans cannot always quantitively verify every step or long-term outcome.\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 43, 'page_label': '37'}, page_content='3 Methodology\\nAs a result, over time, research evolved towards methods like RLAIF and RLVR, where\\nAI-generated or programmatically verifiable feedback replaces human evaluation. GRPO\\nbuilds on this idea by using structured, verifiable rewards that check whether outputs\\nfollow the correct format and/or solve a problem step by step. This evolution has made\\nreinforcement learning scalable, cheaper, and more reliable for reasoning tasks. In our\\nFigure 11:This figure from the GRPO paper (Z. Shao, P. Wang, Zhu, et al. 2024)\\ndemonstrates the difference between Proximal Policy Optimization (PPO)\\n(existing) and GRPO (new) RL algorithms. It is important to note that,\\nGRPO forgoes the value model instead, estimating baseline from group scores.\\nThis significantly reduces training resources.\\nwork, GRPO fits perfectly because our Blocksworld planning tasks can provide clear,\\nstate-wise rewards through unified-planning, allowing our smaller model to learn\\neffectively without constant human supervision. Furthermore, GRPO uses average\\nrewards over multiple generations as a guiding signal, thus eliminating the need for\\na trainable value model. This makes the training process lighter, and well-suited for\\nour academic experimentation with limited resources. Since its launch, various articles\\nhave been published by the industry on eliciting reasoning in smaller models using\\nthis training algorithm. These articles highlight the potential for improving structured\\nreasoning in planning tasks, and low-resource fine-tuning. We have also utilised these\\narticles as a reference point for this emerging technology.\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 44, 'page_label': '38'}, page_content='3 Methodology\\n3.6.2 Input Data Preparation\\nData Adaptation for GRPO\\nIn a previous section we explain how our generated dataset was converted into input\\nprompts, leading to the final prompt template shown in Figure 3.2.3. For GRPO purposes,\\nwe further alter this prompt template to align with the expected input by the algorithm.\\nOn this note, we add a system prompt inspired by the DeepSeek paper to encourage\\ngeneration of reasoning traces by the model within the <think> </think> tags. These\\nreasoning traces are vital in our GRPO setup because, similar to DeepSeek-R1, they\\nare expected to force the model to \"think\" before forming a final action plan. In our\\ncase, the <think> tags capture the model’s planning steps in Blocksworld, the plan\\nfollowing this can be automatically validated against the simulated state transitions in\\nunified-planning.\\nSystem Prompt for GRPO\\nI am a blocksworld plan generator. I first think about the reasoning\\nprocess in the mind and then provide the user with the plan. The\\nreasoning process and plan are enclosed within <think> </think> and\\n[PLAN] [PLAN END] tags, respectively, i.e., <think> reasoning process\\nhere </think> [PLAN] plan here [PLAN END].\\nFigure 12: System Prompt for GRPO\\nThis system prompt will be appended to our base one-shot prompt template to generate\\ninput prompt for GRPO implementation.5 Furthermore, as discussed in the previous\\nsection outlining SFT, our base model is an instruction fine-tuned model. As a result\\nof which, we use a chat template to format our inputs before giving them to the model.\\nThis template follows a structure similar to how users and assistants interact in a chat.\\nThis wrapping formats the prompt with special tokens: <|user|> and <|assistant|>, to\\nemulate a chat-like conversation between the model and the user.\\nTokenisation Process\\nAs established earlier, we use HuggingFace to deploy language model related tools.\\nAfter adding the necessary instructions and task-specific tokens to the input prompt,\\nwe move on to tokenising our dataset. Unlike SFT, the GRPO setup does not expect\\n5It is to be noted that, even though the expected outcome from the model is a plan in the Blocksworld\\ndomain along with its reasoning behind generating that particular plan, the model only receives a\\nproblem statement followed by a correct plan. Wedo not provideany reasoning traces to the model\\nduring training.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 45, 'page_label': '39'}, page_content='3 Methodology\\ninput–output pairs as model input. As a result, the algorithm does not require symmetry\\nin input sequence lengths, so we do not need to apply padding. However, we do compute\\nthe maximum prompt length and pass it as an important parameter to the trainer. This\\nis done to ensure proper batching and prevent the model from exceeding its context\\nwindow during training. Even though we tokenise our data beforehand using the\\nGemma-3 tokeniser provided by Hugging Face, manual inspection revealed that the\\nGRPO implementation computes input tokens dynamically at training time. Therefore,\\nin addition to pre-tokenised data, we also pass a processor class object that contains the\\ntokeniser to the training pipeline. The trainer’s processor is passed so it can handle\\ntokenisation dynamically.\\nThe exact distribution of tokens, apart from the fixed system prompt, stays the\\nsame as shown in the earlier diagram ( 3.5.2). A more detailed look at scaling complexity\\nshows that token length, batch size, and policy update steps directly affect GRPO\\ntraining speed. Longer inputs increase memory use and computation because the model\\nmust generate outputs, calculate log-probabilities, and compute rewards for every token.\\nChoosing the right maximum prompt length and batch size is important to keep training\\nefficient and avoid running out of GPU memory. A problem we look at in more detail in\\nthe upcoming chapter.\\n3.6.3 Reward Signal Design\\nIn GRPO, the reward signal guides the model towards producing outputs that meet the\\ntask requirements. As eluded to earlier, each generated response is scored by a reward\\nfunction, which in our setup checks for correct output format and task completion. We\\nachieve the former using regular expressions and the latter is accomplished with the help\\nof unified-planning. Exact matches in format reward receive higher rewards, while\\npartial matches receive lower rewards. Furthermore, for plan rewards we follow a process\\nreward strategy instead of a outcome-based reward method. Designing the reward signal\\nrequires care and consideration. Rewards that are too simple and frequent, have been\\nshown to lead to reward hacking. This phenomenon is when the model exploits shortcuts\\nto maximise the score without truly solving the intended task. For example, our model\\ncould repeatedly unstack and stack the same block to produce valid actions, without ever\\nprogressing towards the desired goal state. Similarly, sparse or inconsistent rewards can\\nslow down learning and cause unstable updates.To avoid this, rewards are normalised\\nwithin each batch of completions, and a KL-divergence penalty is used to prevent the\\nmodel from drifting too far from the base policy.\\nThis balanced design ensures that the reward signal is both informative and\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 46, 'page_label': '40'}, page_content='3 Methodology\\nFormat Reward :response should contain\\n<think> reasoning here\\n</think> [PLAN] plangenerated here [PLAN\\nEND]\\nReward\\nfunctions\\nPlan\\nRewards\\ndistance\\nprogressed * 5\\nValid action: Action\\nshould be executable\\nin the current state ofthe game\\n0 - 20 points\\nStep towards goal:\\nAction takes the current\\ngame state towards goalstate\\nNo\\n+ (# valid action x 2) points\\nMAX points: # gold plan actions x 2\\nYes\\nAt least 1 valid \\naction that stepstowards goal state\\n+10 points+15 points\\nBonus\\nRewards\\n0 - 50 points\\nTermination:\\nGoal state isreached\\nModel plan length \\n= gold plan length \\nModel plan length \\n< gold plan length \\nTotal:\\n 0 - 105 points\\nYes\\n+ 20 points\\nNo\\nTerminates?\\n+ 0\\npoints\\n+ 0\\npoints\\n0 - 30/35 points\\nFigure 13:This figure shows the three different reward functions used to guide GRPO\\ntraining of our language model.\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 47, 'page_label': '41'}, page_content='3 Methodology\\nstable, allowing GRPO to effectively guide the model towards generating useful and\\ncorrectly formatted outputs. To this extent, we experiment with multiple reward signal\\nrenditions. Finally, we settle at a reward signal that lies in the range of 0 to 100 or 105\\npoints. The is structure we eventually land on is illustrated in Figure 13.\\nFormat Reward Function\\nLet us first look at the format reward function. Format rewards lies in the range of 0\\nto 20 points, where 0 is the lowest and 20 is the highest value a model response can be\\nawarded by this function. The motivation behind assigning specific values is to encourage\\nthe model to generate clean, well-structured, and as a result easily parsable responses.\\nWe use structured start and end tags as this makes it easier to extract information\\nfrom the output. Since LLMs often generate gibberish or inconsistent formats, having\\nsuch a reward function is especially important in RLVR setups. In our prompt to the\\nmodel (Figure 3.6.2) we have two main sections in the expected output: think and plan.\\nHence, in our format reward design we have awarded points based on output’s structural\\nadherence to these two sections. Additionally, we have deducted points for responses that\\ncontain text within the tags albeit with gibberish around them. This is put in place to\\ndiscourage unnecessary consumption of tokens on nonsensical ramblings that sometimes\\nthese models are prone to do. The following table gives a clear description of possible\\nscenarios with regards to model response structure and points assigned in the respective\\nsituations.\\nTable 3.4: Format rewards bi-furcation\\nGibberish Plan tags Think tags Points\\nNo No No 0\\nNo No Yes 7\\nNo Yes No 3\\nNo Yes Yes 20\\nYes No No 0\\nYes No Yes 5\\nYes Yes No 2\\nYes Yes Yes 15\\nTable 3.5:This table presents the format reward scores assigned to model outputs based\\non the presence or absence of specific response tags. This encourages the\\ngeneration of think traces and promotes well-structured outputs. It simplifies\\nparsing and further allows for a more effective assignment of plan rewards.\\nAs we can see in the figure, production of think traces is rewarded solely through this\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 48, 'page_label': '42'}, page_content='3 Methodology\\nreward function. As a result we notice that in the assigned reward points, generating\\nthink traces is valued more than producing plan sequences in format reward function.\\nThis is a deliberate design choice to encourage the model to reflect and reason before\\ncommitting to an action. This is a method, studies claim, that produces the coveted\\n’AHA’ moment6 in models. We are hoping to evaluate this phenomena’s occurrence for\\nour use case. Based on our above function, to receive a maximum of 20 points, the model\\nneeds to produce reasoning trace, followed by a plan action sequence enclosed within the\\nproper tags and with no other text outside these tags.\\nPlan Reward Function\\nNow, we move on to the plan reward function. This component is responsible for\\nevaluating whether the model-generated action sequences lead toward a valid plan\\nnavigating its way to the desired goal. Everything contained within the start and end\\nplan tags is processed by this function. Both plan validity and checking our distance\\nfrom desired goal state is computed with the help ofunified-planning library. For\\neach model response, we create a new simulation and run the sequence of actions one by\\none. Plan rewards are computed dynamically to allow the model complete flexibility\\nto generate plans that deviate from the gold plan, as long as they achieve the desired\\noutcome. Additionally, it is important to note that we do not rely solely on final\\noutcome rewards. Instead, we follow a process reward strategy. This means that if a\\nmodel-generated response contains six action steps, and only four of them are correct,\\nthe model is still rewarded for those four correct steps. We believe process-based rewards\\nare more suitable for our use case, as they allow us to nudge gradual progress toward the\\ngoal rather than expecting the model to get everything right in one go. This aligns well\\nwith how humans also learn, by improving step by step. The plan reward is used to\\ncheck if the model is producing useful actions that help reach the goal. It can give up to\\n50 points. To get any reward here, the model must begin with a valid action (Figure:\\n14). A valid action means that this action can be performed in the current state of the\\ngame. If there are no valid actions, the model gets 0 points. Once a valid action is\\nestablished, the model earns 2 points per valid action, up to a maximum of twice the\\nnumber of actions in the gold plan. This upper limit is set to clip the possibility of\\ngaining higher rewards through redundant actions like stacking and unstacking the same\\nobject consecutively. Furthermore, each valid action that moves the game state closer to\\nthe desired state, earns 5 points. This movement is discerned by computing the number\\nof actions needed to reach the goal state from our current game state. Notice that we\\n6The ’AHA’ moment refers to a sudden flash of insight or understanding that occurs during reasoning.\\nIn the context of language models, this is a point where the model, after engaging in step-by-step\\nthinking, arrives at a correct or insightful conclusion that it may not have reached otherwise.\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 49, 'page_label': '43'}, page_content='3 Methodology\\nValid Action:Pick up the brown block ❌  \\nUnstack the brown block from on top of the teal block ✅\\nFigure 14:This figure illustrates the same example configuration in our Blocksworld\\ndomain, as seen in a previous section (Figure 4). This time with this example\\nconfiguration we can understand meaning of valid and invalid actions in a\\ngiven state of blocks. To recap quickly: in this configuration, the left state of\\nblocks is initial configuration and the right one is goal configuration. Given\\nthe initial state of blocks, top action is deemed invalid, whereas bottom one is\\nvalid in our domain.\\ndeliberately assign a slightly higher score to distance progressed (Figure 15) compared\\nto valid plan actions. This is done to encourage the model to go beyond just using the\\ncorrect syntax or action format. We want to reward it more for actually planning in a\\nway that moves towards the goal state. After we compute the total plan reward for the\\nmodel’s response, we also calculate the reward for the gold (reference) plan. The model’s\\nscore is then normalised using the gold plan score so that it fits within the set range of 0\\nto 50 points. This helps us fairly compare different responses and ensures consistency\\nacross examples.\\nThis reward is structured in a way to help guide the model to not only make\\ncorrect moves but also to move in the right direction, step by step, toward the final goal.\\nIt avoids rewarding random or irrelevant actions and its designed to ensure the plan is\\nactually useful in solving the task.\\nBonus Reward Function\\nFinally, we can decode the bonus reward function. This function is set in place to reward\\nthe model separately, and heftily for reaching the desired goal state. This extra reward\\nhopefully helps elicit complete, optimal and correct plans by making successful end\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 50, 'page_label': '44'}, page_content='3 Methodology\\n  Model plan: \\n✅  Unstack the brown block from on top of the teal block\\n✅  Put down the brown block✅  Unstack the teal block from on top of the violet block❌  Pick up the violet block\\n# Actions     to goal:4\\n32-\\nDistance progressed = 4 - 2 = 2\\nDistance Progressed\\nFigure 15:In the same initial and goal configuration of blocks, we now showcase a clear\\ndepiction of how distance progressed metric is computed for a model response.\\nAs the valid actions are run on a simulated game, we compute number of\\nactions to the goal state for each. Finally, we take the difference between\\nactions to goal from last valid action and initial state. This difference is\\ndistance progressed by acting on model generated plan.\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 51, 'page_label': '45'}, page_content='3 Methodology\\nstates more desirable than partially correct intermediate steps.\\nThis function can give up to thirty five points. When a generated plan termi-\\nnates successfully, model receives a positive score of twenty; otherwise, it is awarded no\\npoints. Additionally, for these terminated plans we check their optimality in number of\\nactions. When the number of actions used to reach termination (i.e desired goal state)\\nis consistent with those in gold plan, the model is awarded an additional ten points.\\nHowever, in presumably rare occasions7 when the model plan outperforms the gold plan\\nwith regards to optimal plan length, it is able to garner an additional fifteen points\\ninstead of ten points. Our motivation behind setting such values for this function is to\\ngo beyond plan validation, we intend to encourage the model to strive for optimality.\\n3.6.4 Training Setup\\nOur model selection remains consistent across all three setups. As detailed in the model\\nselection section (Section 3.3), we utilise thegemma-3b-12b-it model from Hugging\\nFace. Consistent with our previous approach in SFT, we apply PEFT using LoRA. For\\ntraining, we employ Hugging Face’s implementation of GRPO through the transformers\\nand trl libraries.\\nRL: Memory and Compute Considerations\\nInitial experiments are conducted on NVIDIA A40 GPUs, which offer limited memory\\nand compute capacity. To mitigate these constraints, we use LoRA to reduce training\\nresource requirements. However, we quickly observe that GRPO introduces novel\\nmemory consumption requirements and patterns. Unlike standard training, this\\nalgorithm requires the model to generate multiple outputs per input. These inputs are\\nthen evaluated, based on which our model makes an update in the weights and biases.\\nThis increases computational and storage demands. To address this, we initially utilise\\nUnsloth, an optimisation library. Unsloth is a Python library designed to accelerate and\\noptimise fine-tuning of LLMs through various optimisation strategies. It claims to make\\nmodel training and fine-tuning extremely fast, memory-efficient, and easy, especially on\\nlimited hardware like ours. These features met with our initial need for fine-tuning the\\nmodel with reduced memory requirements and faster training times.\\nDespite its advantages, with Unsloth, we observe unpredictable memory con-\\nsumption and performance degradation due to aggressive optimisations. As a result\\n7In rare cases, it is possible that the generated gold plan has redundancy. This phenomenon is noticed\\nmore as the number of block-set size and problem complexity increases.\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 52, 'page_label': '46'}, page_content='3 Methodology\\nbeyond our dummy experiments, we decide to drop Unsloth and switch to a different\\nsolution. To overcome pur existing limitations, we transition to a more powerful system\\nequipped with NVIDIA H100 GPU. On the H100, we conduct PEFT training without\\nUnsloth. We even put a few memory optimisation techniques that we observe are\\nmissing from our version of trl’s GRPO trainer. This transition allows us to handle\\nGRPO training more efficiently, reducing memory bottlenecks and improving training\\nspeed. The enhanced hardware resources are particularly beneficial, as reinforcement\\nlearning-based methods like GRPO are generally slower than standard supervised\\ntraining.\\nPolicy Updates in GRPO: Brief Overview\\nIn Group Relative Policy Optimization, the update step is a crucial phase that refines\\nthe policy to improve performance on the target task. Unlike traditional algorithms\\nsuch as PPO, GRPO eliminates the need for a separate value function or critic network.\\nInstead, the algorithm leverages the relative ranking of rewards across a group of sampled\\ntrajectories.\\nDuring each train iteration, the model samples a batch of trajectories or generations by\\ninteracting with the environment using the current policy model. For each generation,\\nGRPO computes the total reward. It then compares the rewards within the batch or\\ngroup, ranking plan rewards according to their performance rewards. The policy update\\nis performed using a group-wise relative advantage, which encourages the policy to\\nincrease the probability of actions leading to higher rewards within the sampled set.\\nθ′= θ+ α∇θEτ∼πθ[AGR(τ)] (3.2)\\nMathematically, the policy parameters are adjusted using a gradient ascent step based\\non these relative advantages. This approach helps stabilize training by focusing updates\\non action-plans that are empirically better within each group, rather than relying on\\nabsolute value estimates. As a result, the agent can efficiently learn improved behaviours,\\neven in environments where reward signals are sparse or noisy. Furthermore, as eluded\\nto in the reward signal design section, the algorithm applies these updates conservatively\\nusing KL-divergence regularisation method to avoid steering too far away from existing\\npolicy.\\nLGRPO(θ) =− 1\\n∑G\\ni=1 |oi|\\nG∑\\ni=1\\n|oi|∑\\nt=1\\n[\\nπθ(oi,t|q, oi,<t)\\n[πθ(oi,t|q, oi,<t)]no grad\\nˆAi,t−βDKL [πθ ∥πref]\\n]\\n(3.3)\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 53, 'page_label': '47'}, page_content='3 Methodology\\nThe exact values for key hyperparameters like the number of training epochs, batch\\nsize, learning rate, number of generations, gradient accumulation steps, weight decay etc.\\nwill be discussed in the following chapter. Choices are made to balance performance\\nand memory usage, especially considering that we train on relatively long sequences.\\nFurthermore, we will also discuss strategies employed for efficient train runs on limited\\nhardware.\\nWhile this section covers the base setup, and gives an understanding of the\\nworkings of GRPO; further specifications with regard to experimentations are discussed\\nin the Experimentation and Results chapter 4.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 54, 'page_label': '48'}, page_content='4 Experimentation and Results\\nGRPO: memory optmization techniques, maybe put in limitations\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 55, 'page_label': '49'}, page_content='5 Final discussion, Limitations and\\nFuture Work\\n49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 56, 'page_label': '50'}, page_content='Acronyms\\nAI Artificial Intelligence. 1, 2\\nCoT Chain of Thought. 2, 4\\nGPT Generative Pre-Trained Transformer. 1\\nGRPO Group Relative Policy Optimization. 2, 36, 45–47\\nLLM Large Language Model. 1, 2, 13, 14, 16, 30, 35, 41, 45\\nLoRA Low-Rank Adaptation. 34, 35, 45\\nLRM Language Reasoning Model. 1, 2\\nNL Natural Language. 16, 18\\nPEFT Parameter-Efficient Fine-Tuning. 34, 35, 45\\nPPO Proximal Policy Optimization. 37, 46, 51\\nRL Reinforcement Learning. 2, 13\\nRLAIF Reinforcement Learning with AI Feedback. 36\\nRLHF Reinforcement Learning from Human Feedback. 36\\nRLVR Reinforcement Learning with Verified Rewards. 36\\nSFT Supervised Fine-Tuning. 2, 13, 30, 31, 33, 35, 45\\n50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 57, 'page_label': '51'}, page_content='List of Figures\\n1 PDDL representation of the Blocksworld domain. We start with describing\\nthe domain language itself in the top-left section of the image. Followed-up\\nwith an example problem instance in the box below the domain language\\ndescription. After this is given a sample plan to reach the desired goal\\nstate. Enlisted actions in this plan are acted upon in the environment on\\nthe right. The environment state representation has been annotated with\\ninitial and goal state. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2 Flowchart for generating Blocksworld initial–goal state pairs. . . . . . . . 15\\n3 Visualisation of example block configuration with initial state (left) to\\ngoal state (right) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4 Natural Language representation of example state configuration . . . . . . 17\\n5 One-shot prompt template used for plan generation . . . . . . . . . . . . . 19\\n6 Overall % of dataset composition based on block size . . . . . . . . . . . . 21\\n7 Train, validation, and test distribution for 3- and 4-block problems . . . . 22\\n8 This figure illustrates causal language modelling, where the model predicts\\nthe next token in a sequence using all previous tokens as input. . . . . . . 32\\n9 Token length distribution across block-set configuration . . . . . . . . . . 33\\n10 Representation of LoRA, where only A and B matrix weights are trained. 34\\n11 This figure from the GRPO paper (Z. Shao, P. Wang, Zhu, et al. 2024)\\ndemonstrates the difference between PPO (existing) and GRPO (new) RL\\nalgorithms. It is important to note that, GRPO forgoes the value model\\ninstead, estimating baseline from group scores. This significantly reduces\\ntraining resources. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n12 System Prompt for GRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n13 This figure shows the three different reward functions used to guide GRPO\\ntraining of our language model. . . . . . . . . . . . . . . . . . . . . . . . . 40\\n51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 58, 'page_label': '52'}, page_content='List of Figures\\n14 This figure illustrates the same example configuration in our Blocksworld\\ndomain, as seen in a previous section (Figure 4). This time with this\\nexample configuration we can understand meaning of valid and invalid\\nactions in a given state of blocks. To recap quickly: in this configuration,\\nthe left state of blocks is initial configuration and the right one is goal\\nconfiguration. Given the initial state of blocks, top action is deemed\\ninvalid, whereas bottom one is valid in our domain. . . . . . . . . . . . . . 43\\n15 In the same initial and goal configuration of blocks, we now showcase\\na clear depiction of how distance progressed metric is computed for a\\nmodel response. As the valid actions are run on a simulated game, we\\ncompute number of actions to the goal state for each. Finally, we take the\\ndifference between actions to goal from last valid action and initial state.\\nThis difference is distance progressed by acting on model generated plan. 44\\n52'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 59, 'page_label': '53'}, page_content='List of Tables\\n3.1 Aggregated evaluation metrics for different candidate models . . . . . . . 27\\n3.2 Impact of CoT Incantations on Plan Correctness Rate Relative to Baseline\\n(No CoT Prompt) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n3.3 Effect of CoT incantations on plan correctness with Scale . . . . . . . . . 30\\n3.4 Format rewards bi-furcation . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n3.5 This table presents the format reward scores assigned to model outputs\\nbased on the presence or absence of specific response tags. This encourages\\nthe generation of think traces and promotes well-structured outputs. It\\nsimplifies parsing and further allows for a more effective assignment of\\nplan rewards. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 60, 'page_label': '54'}, page_content='Bibliography\\nAhn, M., A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakr-\\nishnan, K. Hausman, A. Herzog, and et al. (2022). “Do As I Can, Not As I Say:\\nGrounding Language in Robotic Affordances.” In:arXiv preprint arXiv:2204.01691.\\nAsai, M. and A. Fukunaga (2018). “Classical planning in deep latent space: Bridging the\\nsubsymbolic-symbolic boundary.” In:Proceedings of the AAAI Conference on Artificial\\nIntelligence. Vol. 32. 1.\\nChevalier-Boisvert, M., D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, T. H. Nguyen,\\nand Y. Bengio (2019). “BabyAI: A Platform for Studying the Sample Efficiency of\\nGrounded Language Learning.” In:International Conference on Learning Representa-\\ntions (ICLR).\\nCobbe, K., V. Kosaraju, M. Bavarian, M. Chen, H. Jun, Ł. Kaiser, M. Plappert, J.\\nTworek, J. Hilton, J. Schulman, M. Knight, S. Witty, and D. Amodei (2021). “Training\\nVerifiers to Solve Math Word Problems.” In:arXiv preprint arXiv:2110.14168.\\nCui, G., L. Yuan, N. Ding, G. Yao, B. He, W. Zhu, Y. Ni, G. Xie, R. Xie, Y. Lin, Z.\\nLiu, and M. Sun (2024). “Ultrafeedback: Boosting Language Models with Scaled AI\\nFeedback.” In: To appear.\\nDeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, and ... (2025). “DeepSeek-R1:\\nIncentivizing Reasoning Capability in LLMs via Reinforcement Learning.” In:arXiv\\npreprint arXiv:2501.12948.\\nDing, N., Y. Chen, B. Xu, Y. Qin, Z. Zheng, S. Hu, Z. Liu, M. Sun, and B. Zhou\\n(2023). “Enhancing Chat Language Models by Scaling High-Quality Instructional\\nConversations.” In:arXiv preprint arXiv:2305.14233.\\nDu, M., F. He, N. Zou, D. Tao, and X. Hu (2023).Shortcut Learning of Large Language\\nModels in Natural Language Understanding. arXiv:2208.11857 [cs.CL].\\nFikes, R. E. and N. J. Nilsson (1971). “STRIPS: A new approach to the application of\\ntheorem proving to problem solving.” In:Artificial intelligence2.3-4, pp. 189–208.\\nGehring, J., K. Zheng, J. Copet, V. Mella, T. Cohen, and G. Synnaeve (2025). “RLEF:\\nGrounding Code LLMs in Execution Feedback with Reinforcement Learning.” In:\\nInternational Conference on Learning Representations (ICLR). Spotlight poster.\\nGeva, M., D. Khashabi, T. Khot, A. Sabharwal, and J. Berant (2021). “Did Aristotle\\nUse a Laptop? A Question Answering Benchmark with Implicit Facts and Multi-Hop\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 61, 'page_label': '55'}, page_content='Bibliography\\nReasoning.” In:Proceedings of the 2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics (NAACL), pp. 3214–3226.\\nGuan, L., K. Valmeekam, S. Sreedharan, and S. Kambhampati (2023). “Leveraging Pre-\\ntrained Large Language Models to Construct and Utilize World Models for Model-based\\nTask Planning.” In:arXiv preprint arXiv:2305.14909.\\nHu, E. J., Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, L. Wang, and W. Chen (2023). “LoRA:\\nLow-Rank Adaptation of Large Language Models.” In:arXiv preprint arXiv:2303.10130.\\nHuang, W., P. Abbeel, D. Pathak, and I. Mordatch (2022). “Language Models as Zero-Shot\\nPlanners: Extracting Actionable Knowledge for Embodied Agents.” In:Proceedings of\\nthe 39th International Conference on Machine Learning (ICML). PMLR, pp. 9118–\\n9147.\\nHuang, W., F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I.\\nMordatch, Y. Chebotar, and et al. (2022). “Inner Monologue: Embodied Reasoning\\nthrough Planning with Language Models.” In:arXiv preprint arXiv:2207.05608.\\nKambhampati, S. (Apr. 2024). “Can Large Language Models Reason and Plan?” In:\\nAnnals of the New York Academy of Sciences1534.1, pp. 15–18.doi: 10.1111/nyas.\\n15125.\\nKambhampati, S., K. Valmeekam, M. Marquez, and L. Guan (July 2023).On the Role of\\nLarge Language Models in Planning. Tutorial presented at the International Conference\\non Automated Planning and Scheduling (ICAPS). Prague.\\nKojima, T., S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa (2022). “Large Language\\nModels are Zero-Shot Reasoners.” In:arXiv preprint arXiv:2205.11916.\\nLample, G. and F. Charton (2019). “Deep learning for symbolic mathematics.” In:arXiv\\npreprint arXiv:1912.01412.\\nLi, D., S. Cao, T. Griggs, S. Liu, X. Mo, E. Tang, S. Hegde, K. Hakhamaneshi, S. G. Patil,\\nM. Zaharia, J. E. Gonzalez, and I. Stoica (2025).LLMs Can Easily Learn to Reason\\nfrom Demonstrations Structure, not content, is what matters!arXiv: 2502.07374\\n[cs.AI].\\nLi, M., R. Zhang, X. Chen, and A. Gupta (2023). “Language Models Can Leverage Demon-\\nstration Structure Over Content for Reasoning.” In:arXiv preprint arXiv:2303.06789.\\nLiu, T., W. Xu, W. Huang, Y. Zeng, J. Wang, X. Wang, H. Yang, and J. Li (2025).\\nLogic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language\\nModels. arXiv:2409.17539 [cs.CL].\\nLuo, L., Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu, Y. Zhu, L. Meng, J. Sun,\\nand A. Rastogi (2024). “Improve Mathematical Reasoning in Language Models by\\nAutomated Process Supervision.” In:arXiv preprint arXiv:2406.06592.\\nLuo, R., J. Li, C. Huang, and W. Lu (2025). “Through the Valley: Path to Effective Long\\nCoT Training for Small Language Models.” In:arXiv preprint arXiv:2506.07712.\\n55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 62, 'page_label': '56'}, page_content='Bibliography\\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S.\\nAgarwal, K. Slama, A. Ray, et al. (2022). “Training Language Models to Follow\\nInstructions with Human Feedback.” In:arXiv preprint arXiv:2203.02155.\\nPareja, C., A. Smith, R. Kumar, and J. Lee (2024). “Unveiling the Secret Recipe: A\\nGuide for Supervised Fine-Tuning of Small Language Models.” In:arXiv preprint\\narXiv:2401.12345.\\nPatel, J., A. Marasovic, P. Agrawal, O. Tafjord, P. Clark, and Y. Choi (2021). “Are\\nNLP Models really able to Solve Simple Math Word Problems?” In:Proceedings of\\nthe 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP),\\npp. 3670–3680.\\nPerez, E., L. Ribeiro, D. Kiela, K. Cho, and P. Lewis (2021). “True Few-Shot Learning\\nwith Language Models.” In:arXiv preprint arXiv:2105.11447.\\nQin, Y., X. Li, H. Zou, Y. Liu, S. Xia, Z. Huang, Y. Ye, W. Yuan, H. Liu, Y. Li, and\\nL. Pengfei (2024). “O1 Replication Journey: A Strategic Progress Report – Part 1.” In:\\narXiv preprint arXiv:2410.18982.\\nRaman, S. S., V. Cohen, E. Rosen, I. Idrees, D. Paulius, and S. Tellex (2022). “Plan-\\nning with Large Language Models via Corrective Re-Prompting.” In:arXiv preprint\\narXiv:2211.09935.\\nRobinson, J., C. M. Rytting, and D. Wingate (2023). “Leveraging Large Language Models\\nfor Multiple Choice Question Answering.” In: arXiv:2210.12353 [cs.CL].\\nShao, Z., P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu,\\nand D. Guo (2024).DeepSeekMath: Pushing the Limits of Mathematical Reasoning in\\nOpen Language Models. arXiv:2402.03300 [cs.CL].\\nShridhar, M., X. Yuan, M.-A. Côté, Y. Bisk, A. Trischler, and M. Hausknecht (2020).\\n“ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.”\\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP).\\nSinha, K., S. Sodhani, S. Rajeswar, J. Binas, H. Larochelle, and J. Pineau (2019).\\n“CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text.” In:Proceed-\\nings of the 2019 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP).\\nSrivastava, A., A. Rastogi, K. Rao, et al. (2022). “Beyond the Imitation Game: Quan-\\ntifying and extrapolating the capabilities of language models.” In:arXiv preprint\\narXiv:2206.04615.\\nStiennon, N., L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, P.\\nChristiano, and J. Schulman (2020). “Learning to summarize with human feedback.” In:\\nAdvances in Neural Information Processing Systems (NeurIPS). Vol. 33, pp. 3008–3021.\\n56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 63, 'page_label': '57'}, page_content='Bibliography\\nSu, Y., D. Yu, L. Song, J. Li, H. Mi, Z. Tu, M. Zhang, and D. Yu (2025). “Crossing the\\nReward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains.” In:\\narXiv preprint arXiv:2503.23829.\\nTrivedi, H., N. F. Rajani, C. Xiong, M. Faruqui, and M. Iyyer (2022). “MuSiQue: Multihop\\nQuestions via Single-hop Question Composition.” In:Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics (ACL).\\nValmeekam, K., M. Marquez, and S. Kambhampati (2023). “Can Large Language Models\\nReally Improve by Self-Critiquing Their Own Plans?” In:NeurIPS 2023 Foundation\\nModels for Decision Making Workshop.\\nValmeekam, K., M. Marquez, A. Olmo, S. Sreedharan, and S. Kambhampati (2023).\\n“PlanBench: An Extensible Benchmark for Evaluating Large Language Models on\\nPlanning and Reasoning about Change.” In:Advances in Neural Information Processing\\nSystems 36. NeurIPS 2023 Datasets & Benchmarks Track, pp. 38975–38987.\\nValmeekam, K., M. Marquez, S. Sreedharan, and S. Kambhampati (2023). “On the\\nPlanning Abilities of Large Language Models – A Critical Investigation.” In:arXiv\\npreprint arXiv:2305.15771.\\nValmeekam, K., H. Zhan, R. I. Brafman, R. Chitnis, and S. Choudhury (2022). “Large\\nlanguage models as zero-shot planners for robot manipulation.” In:Conference on\\nRobot Learning. PMLR, pp. 1502–1515.\\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\\nI. Polosukhin (2017). “Attention Is All You Need.” In:arXiv preprint arXiv:1706.03762.\\nWang, B. and D. Zhou (2022). “Towards Reasoning without Prompting: Extracting\\nChain-of-Thought from Language Models.” In:arXiv preprint arXiv:2212.10071.\\nWang, X., J. Wei, D. Schuurmans, M. Bosma, E. H. Chi, Q. V. Le, and D. Zhou (2023).\\n“Self-Consistency Improves Chain of Thought Reasoning in Language Models.” In:\\narXiv preprint arXiv:2203.11171.\\nWang, Y., Q. Yang, Z. Zeng, L. Ren, L. Liu, B. Peng, H. Cheng, X. He, K. Wang, J.\\nGao, W. Chen, S. Wang, S. S. Du, and Y. Shen (2025). “Reinforcement Learning for\\nReasoning in Large Language Models with One Training Example.” In:arXiv preprint\\narXiv:2504.20571.\\nWei, J., X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and\\nD. Zhou (2022). “Chain of Thought Prompting Elicits Reasoning in Large Language\\nModels.” In:arXiv preprint arXiv:2201.11903.\\nWeng, Y., H. Guo, H. Liu, H. Zhang, Y. Yang, Z. Wang, Y. Hou, and S. Zhou (2022).\\n“Large Language Models are Better Reasoners with Self-Verification.” In:arXiv preprint\\narXiv:2212.09561.\\nWu, Y., Z. Sun, H. Yuan, K. Ji, Y. Yang, and Q. Gu (2024). “Self-Play Preference\\nOptimization for Language Model Alignment.” In:arXiv preprint arXiv:2405.00675.\\n57'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 64, 'page_label': '58'}, page_content='Bibliography\\nXie, C., Y. Huang, C. Zhang, D. Yu, X. Chen, B. Y. Lin, B. Li, B. Ghazi, and R.\\nKumar (2025). On Memorization of Large Language Models in Logical Reasoning.\\narXiv: 2410.23123 [cs.CL].\\nXie, Y., A. Goyal, W. Zheng, M.-Y. Kan, T. P. Lillicrap, K. Kawaguchi, and M. Shieh\\n(2024). “Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning.”\\nIn: arXiv preprint arXiv:2405.00451.\\nYang, J., H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu (2023).\\n“Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.” In:\\narXiv: 2304.13712 [cs.CL].\\nYang, Z., P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D.\\nManning (2018). “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\\nAnswering.” In:Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pp. 2369–2380.\\nYao, S., J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao (2023).\\n“ReAct: Synergizing Reasoning and Acting in Language Models.” In:The Eleventh\\nInternational Conference on Learning Representations (ICLR).\\nYe, X. and G. Durrett (2022). “The Unreliability of Explanations in Few-Shot Prompting\\nfor Textual Reasoning.” In:Proceedings of the 2022 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), pp. 10599–10614.\\nYeo, E., Y. Tong, M. Niu, G. Neubig, and X. Yue (2025). “Demystifying Long Chain-of-\\nThought Reasoning in LLMs.” In:arXiv preprint arXiv:2502.03373.\\nYuan, L., W. Li, H. Chen, G. Cui, N. Ding, K. Zhang, B. Zhou, Z. Liu, and H. Peng (2024).\\n“Free Process Rewards without Process Labels.” In:arXiv preprint arXiv:2412.01981.\\nYue, Y., Z. Chen, R. Lu, A. Zhao, Z. Wang, Y. Yue, S. Song, and G. Huang (2025).\\n“Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond\\nthe Base Model?” In:arXiv preprint arXiv:2504.13837.\\nZhang, B., Y. Shao, Y. Deng, P. Shi, and X. Lin (2022). “Automatic Chain of Thought\\nPrompting in Large Language Models.” In:arXiv preprint arXiv:2210.03493.\\nZhang, L., B. Wang, X. Qiu, S. Reddy, and A. Agrawal (2025).REARANK: Reasoning\\nRe-ranking Agent via Reinforcement Learning. arXiv:2505.20046 [cs.IR].\\nZheng, C., Z. Liu, E. Xie, Z. Li, and Y. Li (2024).Progressive-Hint Prompting Improves\\nReasoning in Large Language Models. arXiv:2304.09797 [cs.CL].\\nZhou, Y., C. Paduraru, A. Boteanu, L. P. Kaelbling, and T. Lozano-Pérez (2020).\\n“Learning latent landmarks for planning from pixels.” In:Conference on Robot Learning.\\nPMLR, pp. 233–245.\\n58')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"thesis.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8fa0a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Answer the question based on the given context. Incase, you \n",
      "cannot answer the question based SOLELY on the context, reply \"I dont know\".\n",
      "\n",
      "Context: Hi I am susan.\n",
      "Question: What is my name?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = '''Answer the question based on the given context. Incase, you \n",
    "cannot answer the question based SOLELY on the context, reply \"I dont know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt.format(\n",
    "    context=\"Hi I am susan.\",\n",
    "    question=\"What is my name?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd54ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what Susan's name is based on the given context. The user provided a conversation where they introduced themselves as Susan and asked, \"Was ist meine name?\" in German. \n",
      "\n",
      "First, I'll break down the question. It seems like it's asking for the user's name in German. Since the user mentioned they're called \"Susan,\" that should be straightforward.\n",
      "\n",
      "Wait, but sometimes names can have accents or different spellings. In this case, Susan is already a common spelling, so the answer should be Susan without any changes.\n",
      "\n",
      "I don't think there are other context clues because the previous message was about a response based on the given context, which only includes that conversation. So, I should stick to that information.\n",
      "\n",
      "Also, since it's a direct question and the context is clear, I can confidently say Susan.\n",
      "</think>\n",
      "\n",
      "Susan\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "print(chain.invoke({\n",
    "    \"context\":\"Hi I am susan.\",\n",
    "    \"question\": \"Was ist meine name?\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53f4bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f5bc9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 20, 'page_label': '14'}, page_content='3 Methodology\\na custom dataset within the Blocksworld domain. Usually, problem statements from\\nBlocksworlds domain consist of a set of named blocks that can be stacked on top of one\\nanother or placed on a table. The goal is to move the blocks from an initial configuration\\nto a target configuration using a sequence of valid actions, namely:pick-up, stack,\\nput-down, unstack. Any action can only be performed after required pre-conditions\\nhave been met, upon performing the action the relevant state variables are updated\\naccordingly to reflect the new configuration. Though the rules are simple, solving a\\nBlocksworld task often requires several steps of planning, making it a useful test-bed\\nfor evaluating how well models can reason through intermediate goals to reach a final state.\\nWe are, by no means, the first ones to use Blocksworld domain problems for test-\\ning reasoning abilities of models. The domain has been widely used in classical planning\\nresearch (Fikes and Nilsson 1971), and more recently in learning-based approaches where\\nmodels attempt to generate action sequences or reason through symbolic transitions\\n(Asai and Fukunaga 2018; Y. Zhou, Paduraru, Boteanu, et al. 2020). It has also featured\\nin neurosymbolic settings that aim to integrate neural networks with planning algorithms\\n(Valmeekam, Zhan, Brafman, et al. 2022). Along with program synthesis tasks requiring\\ninterpretable intermediate steps (Lample and Charton 2019). This consistent use across\\ndecades and methodologies makes Blocksworld a reliable benchmark for evaluating rea-\\nsoning models. A more detailed assessment on this topic can be found in the chapter\\npertaining to related work in chapter 2.\\n3.2.2 Blocksworld Dataset Generation\\nAs the blocksworld domain is well-known there are pre-exisitng datasets that have been\\nused for various studies, as mentioned above. This is freely available to use for academic\\npurposes. Although there’s different versions of this dataset available, they are not suffi-\\nciently vast enough for our purposes. We intend to fine-tune LLMs and further proceed\\nwith Reinforcement Learning. Both of these paradigms require an extensive amount of\\ndata points to get ahead of issues such as overfitting and to ensure generalization across\\ndiverse problem configurations. The limited size of available datasets thus would have\\nrestricted the robustness and reliability of our experiments. This emphasizes the need for\\neither synthetic data generation or augmentation methods to support our experimental\\nobjectives.\\nOwing to this insight, we proceed to create our own blocksworld dataset with namely:\\n• 3 bock configurations\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 19, 'page_label': '13'}, page_content='3 Methodology\\n3.1 Introduction\\nIn this chapter we begin by presenting a brief overview of the dataset generation, outlining\\nour motivation behind it, the process we follow and finally the resulting distribution.\\nThis is followed by the model selection process and criterion behind the eventual selec-\\ntion. Since evaluating an LLM response requires checking action correctness and plan\\nvalidity, next we describe how responses are processed. For this, we rely heavily on the\\nunified-planning environment which we will briefly touch upon.\\nThis chapter further elaborates on the methodology we follow in this work across the\\nthree main experimental setups: prompting, Supervised Fine-Tuning, and Reinforcement\\nLearning. The objective is to understand how each of these approaches contributes to\\nreasoning and plan generation in the Blocksworld domain. To this regard, we describe the\\nSFT setup, including pre-processing steps, model input formatting and training details.\\nLastly, we look at the RL setup, more specifically Group Relative Policy Optimization,\\nexplaining how the environment, reward structure, and learning objectives are defined.\\nAdditionally, we also outline the structure and role of different variations of one-shot\\nprompts in producing action plans from language models.\\n3.2 Dataset Generation\\nEach of the following sections builds upon the dataset introduced in its previous section\\nand, together they provide a basis to evaluate the model’s planning ability.\\n3.2.1 Introduction\\nBlocksworld is a legacy environment used to study planning and reasoning. It is a\\nwell-defined domain and yet it contains enough complexity to challenge reasoning models.\\nFor example, through sub-goals that can conflict with one another and tasks that natu-\\nrally break down into smaller sub-problems. Given that the objective of our research\\nis to optimise the reasoning capabilities of the selected model, we choose to generate\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 13, 'page_label': '7'}, page_content='2 Literature Survey\\nthe model (Y. Xie, Goyal, W. Zheng, et al. 2024).\\nAnother upcoming way to make use of external verifiers is through Reinforcement\\nLearning with Verified Rewards (RLVR) (Su, Yu, Song, et al. 2025; Yue, Z. Chen, R. Lu,\\net al. 2025; Y. Wang, Q. Yang, Zeng, et al. 2025). In this setup, there is no separate\\ntrained reward model. The signal is directly gathered from an existing deterministic\\nheuristic. This is an approach we also adopt and will discuss in more detail later. It\\nworks well in our case because we are working with classical planning problems, where\\nestablished libraries can be used to evaluate plans reliably. A notable concern with RLVR,\\nas pointed out by existing work, is that while it improves sampling efficiency, it does\\nnot significantly expand underlying reasoning abilities of the base model (Yue, Z. Chen,\\nR. Lu, et al. 2025). Whether we reach the coveted ’AHA’ moment as claimed by the\\nDeepSeek team with this approach, still remains an open question.\\n2.3 Evaluating Reasoning in Language Reasoning Models\\n(LRM)\\nIn previous sections, we briefly touched upon the question of LLMs genuinely reason, or\\nmerely pattern-match against their training data. This section delves deeper into this\\nquestion and explores results seen using different methodologies already proposed in the\\nexisting literature to probe reasoning abilities in both LLMs and their more new and\\nimproved version: Language Reasoning Model (LRM).\\n2.3.1 LLMs: Can they plan?\\nCentral topics of discussion in this paper is to test whether LLMs are capable of planning.\\nAs seen earlier, while LLMs have demonstrated strong performance on a range of tasks,\\nthere is still scepticism on their planning abilities. Some even deem the models to simply\\nbe universal approximate retrieval n-gram models (ref:can llms plan?). This argument\\nclaims that LLMs are not solving tasks with guarantee and rather retrieving approximate\\nguesses for solution. To tackle this scepticism, studies have attempted to put critique\\ninto the training loop. The role of generating this critique, was also assigned to an\\nLLM. In such scenarios where LLMs are encouraged to reason through self-critiquing\\n(Valmeekam, Marquez, and Kambhampati 2023; Weng, H. Guo, H. Liu, et al. 2022) while\\none group claims that the performance improves, another experienced a stark opposite.\\nThis divergence highlights that the effectiveness of self-critiquing, and more broadly, the\\nplanning capabilities of LLMs, can vary significantly depending on the task formulation,\\nmodel architecture, training objective, and evaluation method. Therefore, there have\\nbeen continued efforts made to improve the reasoning abilities of LLMs and consequently\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-08T14:14:32+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-08T14:14:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'thesis.pdf', 'total_pages': 65, 'page': 49, 'page_label': '43'}, page_content='3 Methodology\\nValid Action:Pick up the brown block ❌  \\nUnstack the brown block from on top of the teal block ✅\\nFigure 14:This figure illustrates the same example configuration in our Blocksworld\\ndomain, as seen in a previous section (Figure 4). This time with this example\\nconfiguration we can understand meaning of valid and invalid actions in a\\ngiven state of blocks. To recap quickly: in this configuration, the left state of\\nblocks is initial configuration and the right one is goal configuration. Given\\nthe initial state of blocks, top action is deemed invalid, whereas bottom one is\\nvalid in our domain.\\ndeliberately assign a slightly higher score to distance progressed (Figure 15) compared\\nto valid plan actions. This is done to encourage the model to go beyond just using the\\ncorrect syntax or action format. We want to reward it more for actually planning in a\\nway that moves towards the goal state. After we compute the total plan reward for the\\nmodel’s response, we also calculate the reward for the gold (reference) plan. The model’s\\nscore is then normalised using the gold plan score so that it fits within the set range of 0\\nto 50 points. This helps us fairly compare different responses and ensures consistency\\nacross examples.\\nThis reward is structured in a way to help guide the model to not only make\\ncorrect moves but also to move in the right direction, step by step, toward the final goal.\\nIt avoids rewarding random or irrelevant actions and its designed to ensure the plan is\\nactually useful in solving the task.\\nBonus Reward Function\\nFinally, we can decode the bonus reward function. This function is set in place to reward\\nthe model separately, and heftily for reaching the desired goal state. This extra reward\\nhopefully helps elicit complete, optimal and correct plans by making successful end\\n43')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"reinforcement learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea6f9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"How and why do I use reinforcement learning in the project.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8705391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = ( \n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | model\n",
    "    | parser \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fe46040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how and why I used reinforcement learning (RL) in my project. Let's break this down step by step.\n",
      "\n",
      "First, looking at the context provided, it seems that the project was about using small models like GPT-3, which are known for their inference speeds but sometimes struggle with reasoning tasks because they don't have much internal structure or mechanisms to think through problems themselves.\n",
      "\n",
      "I remember seeing in the document that they used an instruction-fine-tuned 12B parameter model. This model was likely being trained on a dataset called Blocksworld. The key thing about this setup is that it's a smaller model compared to larger ones like OpenAI's o1 or DeepSeek-R1, which are more powerful because they have more parameters and a more complex architecture.\n",
      "\n",
      "The user mentioned that their goal was to improve reasoning without relying heavily on external verifiers. They were using the Blocksworld domain as their benchmark. In the literature survey section, there's a reference to Valmeekam et al., 2023, which evaluates various models including LLaMA and Gemini. This study showed that while smaller models like GPT-4 can perform well on reasoning tasks with many steps, they often struggle in real-world scenarios because they don't develop problem-solving strategies on their own.\n",
      "\n",
      "So, if the user's project is about training a small model without an external verifier but aiming for good performance, RL might be the way to go. In the context where RL was introduced, it was suggested that combining an LLM with an external domain-specific solver could enhance reasoning by refining and sampling the output chain of thought.\n",
      "\n",
      "Thinking back, their model wasn't just about inferring tokens; they were trying to produce a coherent Chain-of-Through (CoT). So, using RL allowed the model not only to generate more accurate answers but also to refine these chains, making them more coherent and effective. This approach helped address some limitations seen in larger models that relied on external verifiers without internal problem-solving mechanisms.\n",
      "\n",
      "Therefore, I think they used RL because it provided a way to improve both accuracy and coherence of the reasoning output by sampling and refining the CoT traces. This method was aimed at enhancing performance on tasks like plan generation within the Blocksworld domain.\n",
      "</think>\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "In their project, the user employed **reinforcement learning (RL)** to enhance the reasoning capabilities of a small model trained on the Blocksworld domain. RL allowed the model to not only generate accurate answers but also refine and improve the CoT traces, addressing limitations in larger models that relied on external verifiers without internal problem-solving mechanisms. This approach aimed to address challenges seen in larger models by providing more coherent and effective reasoning outputs.\n",
      "\n",
      "**Answer:** The project used **reinforcement learning (RL)** to enhance a small model's reasoning performance within the Blocksworld domain, focusing on refining CoT traces to improve both accuracy and coherence.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a61b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
